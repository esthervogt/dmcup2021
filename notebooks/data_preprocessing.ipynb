{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Author: Esther Vogt\n",
    "# Creation Date: 11.05.2021\n",
    "# Purpose: Pre-processing of raw data + external data / Generation of header-itemset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Dos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Backlog:\n",
    "# - match google data?\n",
    "\n",
    "# Todo:\n",
    "# - description embeddings @Estelle\n",
    "\n",
    "# - clean page numbers @Esther\n",
    "\n",
    "# In Progress:\n",
    "\n",
    "\n",
    "# Done:\n",
    "# - titel preprocessing: stop word removal / Kommentare wie taschenbuch/editions etc. @Esther\n",
    "# - create superset @Esther \n",
    "# - pull main topic and sub topic together @Esther\n",
    "# - sentence embeddings @Estelle\n",
    "# - clean up script a bit @Esther (22.05.2021)\n",
    "# - translate header translation tags @Esther\n",
    "# - clean header translation tags @Esther\n",
    "# - check languages of books in evaluations_df @Esther\n",
    "# - run pre-processing and header-set generation again -> missing items ! @Esther\n",
    "# - make translations per language (only select top 5?) @Esther"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings & User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Imports & Settings\n",
    "########################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "# import pycountry\n",
    "from pandas.core.common import flatten\n",
    "from functools import reduce\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# identification of language used in descriptions\n",
    "import langid\n",
    "\n",
    "# # translation of descriptions\n",
    "# from nltk.corpus import stopwords \n",
    "# from nltk.tokenize import word_tokenize \n",
    "# from transformers import MarianTokenizer, MarianMTModel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# allow display of all rows (with scrollbar)\n",
    "pd.set_option(\"display.max_rows\", 10) #pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# User Input\n",
    "########################################################################################################################\n",
    "\n",
    "# get todays date as str\n",
    "today_str = datetime.today().strftime(\"%Y%m%d\")\n",
    "# today_str = '20210525'\n",
    "\n",
    "# source data file paths\n",
    "transactions_path = '../data/external/transactions.csv'\n",
    "evaluation_path = '../data/external/evaluation.csv'\n",
    "items_path = '../data/external/items.csv'\n",
    "subject_cats_0_path = '../data/external/subject_cats_0.csv'\n",
    "gbooks_path = '../data/external/gbooks_final.json'\n",
    "\n",
    "# interim file paths: file storing language flag for item descriptions\n",
    "description_lang_path = '../data/interim/20210519header_items_description_lang.feather'\n",
    "description_lang_missing_items_path = '../data/interim/missing_header_items_description_lang.feather'\n",
    "description_trans_path = '../data/interim/'\n",
    "language_flg_trans_path = '../data/interim/language_flags_translations.feather'\n",
    "\n",
    "# pre-processed data file paths (incl. language flags)\n",
    "transactions_path_pp = '../data/processed/transactions_pp.csv'\n",
    "items_path_pp = f'../data/processed/{today_str}_items_df.csv'\n",
    "header_items_path_pp = f'../data/processed/{today_str}_header_items_df.csv'\n",
    "header_items_lookup_path_pp = f'../data/processed/{today_str}_header_items_lookup_df.csv'\n",
    "\n",
    "gbooks_volumeInfo_path_pp = '../data/processed/gbooks_volumeInfo_pp.feather'\n",
    "\n",
    "header_items_20210517_path = '../data/processed/20210517_header_items_df.csv'\n",
    "header_items_20210519_path = '../data/processed/20210519header_items_df.csv'\n",
    "header_items_20210519_path = '../data/processed/20210519header_items_df.csv'\n",
    "header_items_lookup_20210519_path = '../data/processed/20210519header_items_lookup_df.csv'\n",
    "\n",
    "# seaborn color palette\n",
    "palette_blue = \"Blues_d\"\n",
    "dark_blue = \"#011f4b\"\n",
    "middle_blue = \"#005b96\"\n",
    "light_blue = \"#b3cde0\"\n",
    "\n",
    "# determine: re-calculate certain details\n",
    "recompute_lg_flg = False # calculated language flags\n",
    "recompute_desc_lg_flg = False # detect language of description\n",
    "recompute_desc_trans = False # translate descriptions into english\n",
    "\n",
    "load_gbooks_data = False # load df containing details from GoogleAPI\n",
    "recompute_gbooks_volumeInfo = False # volumeInfo per book pulled from GoogleAPI\n",
    "\n",
    "load_thalia_data = True # load Thalia data \n",
    "merge_thalia_data_items_df = True # merge thalia data with items_df\n",
    "\n",
    "recompute_header_set = True # aggregate details in items_df for items with same title\n",
    "load_ppgbooks_data_and_merge_with_header = False # load pre-processed google data and merge with items on header level\n",
    "\n",
    "# set language dictionary to be used throughout the notebook\n",
    "lang_dict = {'en': 'english', 'de':'german', 'es':'spanish', 'fr': 'french', 'it': 'italian', 'pt':'portuguese'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Functions\n",
    "########################################################################################################################\n",
    "\n",
    "def clean_alt_list(list_):\n",
    "#     list_ = list_.replace(', ', ',')\n",
    "    list_ = list_.replace('[', '')\n",
    "    list_ = list_.replace(']', '')\n",
    "    return list_\n",
    "\n",
    "\n",
    "def items_initial_col_processing(items_df, drop_original=True):\n",
    "    # add col: get len of mt string\n",
    "#     items_df['mt_len'] = items_df['main topic'].str.len()\n",
    "\n",
    "    # add col: get first element (top level category) of mt string\n",
    "#     items_df['mt_0'] = items_df['main topic'].str[0]\n",
    "\n",
    "    # add col: main topic as set (and converted back to list)\n",
    "    items_df['mt_cl'] = items_df['main topic'].astype(str).apply(lambda x: list(set(clean_alt_list(x).split(','))))\n",
    "\n",
    "    # adjust subtopics: set to None if subtopics list is empty\n",
    "    items_df['st_cl'] = items_df['subtopics'].astype(str).apply(lambda x: list(set(clean_alt_list(x).split(','))))\n",
    "    items_df.loc[items_df['st_cl']=={''}, 'st_cl'] = None\n",
    "\n",
    "    # add col: unique combination of main and subtopic\n",
    "    items_df['mt_st_cl'] = (items_df['st_cl'] + items_df['mt_cl']) #.apply(set)\n",
    "    \n",
    "    # drop initial topic cols\n",
    "    if drop_original:\n",
    "        items_df = items_df.drop(columns=['main topic', 'subtopics'])\n",
    "    \n",
    "    return items_df\n",
    "\n",
    "\n",
    "def tr_initial_col_processing(transactions_df):\n",
    "    # add col: get click / basket / order flag\n",
    "    transactions_df['click_flg'] = np.where(transactions_df['click'] > 0, 1, 0)\n",
    "    transactions_df['basket_flg'] = np.where(transactions_df['basket'] > 0, 1, 0)\n",
    "    transactions_df['order_flg'] = np.where(transactions_df['order'] > 0, 1, 0) \n",
    "    \n",
    "    return transactions_df\n",
    "\n",
    "\n",
    "def extract_gbook_volumeInfo(data, target_keys):\n",
    "\n",
    "    # initialize final details df\n",
    "    volumeInfo_df = pd.DataFrame()\n",
    "    total = len(data)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "    \n",
    "        # print progress report\n",
    "        if int(index%1000) == 0:\n",
    "            print(f'{index}/{total}')\n",
    "    \n",
    "        # extract volumInfo if given\n",
    "        if row[\"items\"]:\n",
    "            for item in row[\"items\"]:\n",
    "\n",
    "                available_keys = list(item['volumeInfo'].keys())\n",
    "    #             print(f'available_keys: {available_keys}')\n",
    "\n",
    "                extraction_keys = list(frozenset(available_keys).intersection(target_keys))\n",
    "    #             print(f'extraction keys: {extraction_keys}')\n",
    "\n",
    "                volumeInfo_item_df = pd.DataFrame(item).loc[extraction_keys,'volumeInfo']\n",
    "                volumeInfo_item_df = pd.DataFrame(volumeInfo_item_df).transpose()\n",
    "                volumeInfo_item_df[\"itemIdx\"] = row[\"itemIdx\"]\n",
    "    #             display(volumeInfo_item_df)\n",
    "    #             print()\n",
    "\n",
    "                volumeInfo_df = pd.concat([volumeInfo_df,volumeInfo_item_df])\n",
    "\n",
    "    # reset index of volumeInfo df\n",
    "    volumeInfo_df.reset_index(inplace=True)\n",
    "    volumeInfo_df = volumeInfo_df.drop(columns='index')  \n",
    "    \n",
    "    return volumeInfo_df \n",
    "\n",
    "\n",
    "def remove_special_characters(list_):\n",
    "#     list_ = re.sub(r'^\\W+', r'', list_) #removes leading non-alphanumerics, e.g. \",william shakespeare\"\n",
    "\n",
    "    # Remove punctuation & special characters\n",
    "#     list_ = re.sub(r'[®★✝•„“»«¡”：‚●♥‘…›<,\\.!¿?\\\"\\(\\)\\'\\:#]','',list_)\n",
    "    list_ = re.sub(r'[®★✝•„“»¡”：‚●♥‘…›<,\\.!¿?\\\"\\(\\)\\'\\:#]','',list_)\n",
    "    list_ = re.sub(r'-',' ',list_)\n",
    "    return list_\n",
    "\n",
    "\n",
    "    \"    list_ = re.sub(r'-',' ',list_)\\n\",\n",
    "\n",
    "\n",
    "def remove_next_sign(list_):\n",
    "#     list_ = re.sub(r'^\\W+', r'', list_) #removes leading non-alphanumerics, e.g. \",william shakespeare\"\n",
    "\n",
    "    # Remove punctuation & special characters\n",
    "    list_ = re.sub(r'[\\n]','',list_)\n",
    "    \n",
    "    return list_\n",
    "\n",
    "def remove_nontitle_substrings(list_):\n",
    "    list_ = str(list_)\n",
    "\n",
    "    # type of book\n",
    "    for book_type in ['taschenbuch','hardcover','hardback']:\n",
    "        list_ = re.sub(f'\\(.*{book_type}.*\\)?','',list_) #remove all content within brackets\n",
    "        list_ = re.sub(f'-\\s*(\\w*\\s*){book_type}.*','',list_)\n",
    "        list_ = re.sub(f':.*{book_type}.*','',list_)\n",
    "        list_ = re.sub(f'(.*{book_type}[\\w\\d\\s]*):','',list_)\n",
    "        list_ = re.sub(f'[(special)(book)(edition)\\s*]*{book_type}\\s*[(special)(book)(edition)\\s*]*','',list_)\n",
    "        list_ = re.sub(f'{book_type}','',list_)\n",
    "        \n",
    "    # (light novel)\n",
    "    list_ = re.sub(f'(light novel)','',list_)\n",
    "    list_ = re.sub(f'\\(novel\\)','',list_)\n",
    "    \n",
    "    # (edition)\n",
    "    list_ = re.sub(f'\\(.*edition.*\\)','',list_)  \n",
    "\n",
    "    return list_\n",
    "\n",
    "\n",
    "def convert_umlaute(list_):\n",
    "    list_ = list_.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\")\n",
    "    return list_\n",
    "\n",
    "\n",
    "def remove_duplicate_whitespace(list_):\n",
    "    list_ = re.sub(f' {2,}','',list_)\n",
    "    return list_\n",
    "\n",
    "\n",
    "def generate_header_set(items_df):\n",
    "    \"\"\"\n",
    "    generates header set of items that combines attributes of several items with same title that e.g. only differ in itemID\n",
    "    or other attributes\n",
    "    > headerID can be used to replace itemID in transactions_df\n",
    "    \"\"\"\n",
    "    # generate header attribute sets from sub-items -> important: generate sets to prevent duplication \n",
    "    header_items_author_df = items_df['author'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    header_items_publisher_df = items_df['publisher'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    header_items_mtst_df = items_df['mt_st_cl'].groupby([items_df.title]).apply(sum).apply(set).reset_index() # get unique list of topics\n",
    "\n",
    "    header_items_language_df = items_df['language'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    header_items_number_pages_df = items_df['number_pages'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    header_items_recommended_age_df = items_df['recommended_age'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    header_items_release_date_df = items_df['release_date'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    header_items_description_df = items_df['description'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    \n",
    "    # compile the list of dataframes you want to merge\n",
    "    header_items_df_lst = [header_items_author_df, header_items_publisher_df, header_items_mtst_df, header_items_language_df,\n",
    "                           header_items_number_pages_df,header_items_recommended_age_df, header_items_release_date_df,\n",
    "                           header_items_description_df ]\n",
    "\n",
    "    # merge all attributes\n",
    "    header_items_df = reduce(lambda left,right: pd.merge(left,right,on=['title'],\n",
    "                                                how='outer'), header_items_df_lst)\n",
    "\n",
    "    # generate new header index\n",
    "    header_items_df = header_items_df.reset_index().rename(columns={'index':'headerID'})\n",
    "\n",
    "    # result inspection\n",
    "    print(f'shape of header_items_df vs. items_df: {header_items_df.shape} vs. {items_df.shape}')\n",
    "    print(f'cnt of duplicate \"title\" in header_df: {(header_items_df[\"title\"].value_counts() > 1).sum()}')\n",
    "\n",
    "#     print(f'\\nconverted df:')\n",
    "#     display(header_items_df[header_items_df['title'].isin(['(Heli-)opolis - Der verhängnisvolle Plan des Weltkoordinators',\n",
    "#                                                    '13 Kings',\n",
    "#                                                    'Ära der Lichtwächter'])].head(5))\n",
    "\n",
    "#     print(f'\\noriginal df:')\n",
    "#     display(items_df[items_df['title'].isin(['(Heli-)opolis - Der verhängnisvolle Plan des Weltkoordinators',\n",
    "#                                                    '13 Kings',\n",
    "#                                                    'Ära der Lichtwächter'])].head(5))\n",
    "\n",
    "    return header_items_df\n",
    "\n",
    "\n",
    "def clean_thalia_language_flags(list_):\n",
    "    list_ = re.sub(f' \\(Untertitel.*\\)','',list_)\n",
    "    list_ = re.sub(f'Untertitel\\s?:','',list_)\n",
    "    list_ = re.sub(r'\\{\\'\\'\\, ','{',list_)\n",
    "    \n",
    "    return list_\n",
    "\n",
    "\n",
    "def clean_set(list_):\n",
    "    list_ = re.sub(r'\\{\\'\\'\\, ','{',list_)\n",
    "    \n",
    "    return list_\n",
    "\n",
    "\n",
    "def unify_set_from_str(list_):\n",
    "    list_ = re.sub(r'[\\{\\}\\' ]','',list_)\n",
    "    \n",
    "    return list_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load & initial pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DMC Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# evaluation items: (1000, 1)\n",
      "# items (1): (78030, 6)\n",
      "# items (2): (78334, 6)\n",
      "items_df after first pre-processing:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemID</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publisher</th>\n",
       "      <th>mt_cl</th>\n",
       "      <th>st_cl</th>\n",
       "      <th>mt_st_cl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21310</td>\n",
       "      <td>Princess Poppy: The Big Mix Up</td>\n",
       "      <td>Janey Louise Jones</td>\n",
       "      <td>Penguin Random House Children's UK</td>\n",
       "      <td>[YFB]</td>\n",
       "      <td>[5AH]</td>\n",
       "      <td>[5AH, YFB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73018</td>\n",
       "      <td>Einfach zeichnen! Step by Step</td>\n",
       "      <td>Wiebke Krabbe</td>\n",
       "      <td>Schwager und Steinlein</td>\n",
       "      <td>[AGZ]</td>\n",
       "      <td>[YBG, 5AJ, WFA, AGZ, YNA, YPA, YBL]</td>\n",
       "      <td>[YBG, 5AJ, WFA, AGZ, YNA, YPA, YBL, AGZ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   itemID                           title              author  \\\n",
       "0   21310  Princess Poppy: The Big Mix Up  Janey Louise Jones   \n",
       "1   73018  Einfach zeichnen! Step by Step       Wiebke Krabbe   \n",
       "\n",
       "                            publisher  mt_cl  \\\n",
       "0  Penguin Random House Children's UK  [YFB]   \n",
       "1              Schwager und Steinlein  [AGZ]   \n",
       "\n",
       "                                 st_cl  \\\n",
       "0                                [5AH]   \n",
       "1  [YBG, 5AJ, WFA, AGZ, YNA, YPA, YBL]   \n",
       "\n",
       "                                   mt_st_cl  \n",
       "0                                [5AH, YFB]  \n",
       "1  [YBG, 5AJ, WFA, AGZ, YNA, YPA, YBL, AGZ]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions_df after first pre-processing:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sessionID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>click</th>\n",
       "      <th>basket</th>\n",
       "      <th>order</th>\n",
       "      <th>click_flg</th>\n",
       "      <th>basket_flg</th>\n",
       "      <th>order_flg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21310</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>73018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sessionID  itemID  click  basket  order  click_flg  basket_flg  order_flg\n",
       "0          0   21310      1       0      0          1           0          0\n",
       "1          1   73018      1       0      0          1           0          0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# Load Data\n",
    "########################################################################################################################\n",
    "\n",
    "# Load the dmc source data\n",
    "\n",
    "# - clicks/baskets/order over a period of 3M\n",
    "# - rows: one transaction for single item\n",
    "transactions_df = pd.read_csv(transactions_path, delimiter='|', sep='.', encoding='utf-8', \n",
    "                              quoting=csv.QUOTE_NONE, error_bad_lines=True)\n",
    "\n",
    "# - list of product ids (subset of products from items_df) to be used for prediction\n",
    "evaluation_df = pd.read_csv(evaluation_path, sep='.', encoding='utf-8', quoting=csv.QUOTE_NONE, error_bad_lines=True)\n",
    "print(f'# evaluation items: {evaluation_df.shape}')\n",
    "\n",
    "items_df_1 = pd.read_csv(items_path, delimiter='|', sep='.', encoding='utf-8')\n",
    "items_df_2 = pd.read_csv(items_path, delimiter='|', sep='.', encoding='utf-8', quoting=csv.QUOTE_NONE, error_bad_lines=True)\n",
    "print(f'# items (1): {items_df_1.shape}')\n",
    "print(f'# items (2): {items_df_2.shape}')\n",
    "\n",
    "# load category lookup table (manually created)\n",
    "subject_cats_0 = pd.read_csv(subject_cats_0_path, delimiter=';', encoding='utf-8', \n",
    "                             quoting=csv.QUOTE_NONE, error_bad_lines=True)\n",
    "\n",
    "########################################################################################################################\n",
    "# Preprocessing for further inspection\n",
    "########################################################################################################################\n",
    "\n",
    "# extract list of base cols\n",
    "initial_cols= list(items_df_1.columns)\n",
    "\n",
    "# add/pre-process cols: items\n",
    "items_df_1 = items_initial_col_processing(items_df_1, drop_original=True)\n",
    "items_df_2 = items_initial_col_processing(items_df_2, drop_original=True)\n",
    "\n",
    "# add/pre-process cols: transactions\n",
    "transactions_df = tr_initial_col_processing(transactions_df)\n",
    "\n",
    "########################################################################################################################\n",
    "# Inspection of dfs after initial pre-processing\n",
    "########################################################################################################################\n",
    "\n",
    "# show dfs after initial pre-processing\n",
    "print(f'items_df after first pre-processing:')\n",
    "display(items_df_1.head(2))\n",
    "\n",
    "print(f'transactions_df after first pre-processing:')\n",
    "display(transactions_df.head(2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# get missing items in items_df\n",
    "items_df_missing = items_df_2.merge(items_df_1[['itemID','title']], on='itemID', how='left')\n",
    "items_df_missing = items_df_missing[items_df_missing['title_y'].isna()].drop(columns='title_y').rename(columns={'title_x': 'title'})\n",
    "items_df_missing = items_df_missing.reset_index()\n",
    "items_df_missing = items_df_missing.drop(columns='index')\n",
    "items_df_missing\n",
    "\n",
    "# store df of missing items\n",
    "# items_df_missing.to_csv(re.sub('items.csv','missing_items.csv',items_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78334, 7)\n"
     ]
    }
   ],
   "source": [
    "# reset items_df_2 to items_df\n",
    "items_df = items_df_2\n",
    "\n",
    "print(items_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processed Header DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71948, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headerID</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publisher</th>\n",
       "      <th>mt_st_cl</th>\n",
       "      <th>recommended_age</th>\n",
       "      <th>release_date</th>\n",
       "      <th>mulit_lang_flg</th>\n",
       "      <th>item_lang_en</th>\n",
       "      <th>description_</th>\n",
       "      <th>description_lang</th>\n",
       "      <th>description_en</th>\n",
       "      <th>rec_age</th>\n",
       "      <th>number_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>and the word became a story</td>\n",
       "      <td>the author</td>\n",
       "      <td>books on demand</td>\n",
       "      <td>FM ,FL ,FN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{Timestamp('2018-04-17 00:00:00')}</td>\n",
       "      <td>0</td>\n",
       "      <td>English</td>\n",
       "      <td>at the beginning was the wordand from the word...</td>\n",
       "      <td>en</td>\n",
       "      <td>at the beginning was the wordand from the word...</td>\n",
       "      <td>adult</td>\n",
       "      <td>{304}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>evvai</td>\n",
       "      <td>cristina polacchini</td>\n",
       "      <td>lulucom</td>\n",
       "      <td>,YFB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{Timestamp('2015-08-01 00:00:00')}</td>\n",
       "      <td>0</td>\n",
       "      <td>Italian</td>\n",
       "      <td>amicizia scuola amore alti e bassi in and out ...</td>\n",
       "      <td>it</td>\n",
       "      <td>friendship school love ups and downs in and ou...</td>\n",
       "      <td>adult</td>\n",
       "      <td>{106}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   headerID                         title               author  \\\n",
       "0         0   and the word became a story           the author   \n",
       "1         1                         evvai  cristina polacchini   \n",
       "\n",
       "         publisher    mt_st_cl recommended_age  \\\n",
       "0  books on demand  FM ,FL ,FN             NaN   \n",
       "1          lulucom        ,YFB             NaN   \n",
       "\n",
       "                         release_date  mulit_lang_flg item_lang_en  \\\n",
       "0  {Timestamp('2018-04-17 00:00:00')}               0      English   \n",
       "1  {Timestamp('2015-08-01 00:00:00')}               0      Italian   \n",
       "\n",
       "                                        description_ description_lang  \\\n",
       "0  at the beginning was the wordand from the word...               en   \n",
       "1  amicizia scuola amore alti e bassi in and out ...               it   \n",
       "\n",
       "                                      description_en rec_age number_pages  \n",
       "0  at the beginning was the wordand from the word...   adult        {304}  \n",
       "1  friendship school love ups and downs in and ou...   adult        {106}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load previously generated header df \n",
    "# - note: use lineterminator='\\n' -> otherwise yields parser error\n",
    "header_items_df = pd.read_csv(header_items_path_pp, encoding='utf-8') #, lineterminator='\\n')\n",
    "print(header_items_df.shape)\n",
    "\n",
    "# # load lookup\n",
    "# header_items_lookup_df = pd.read_csv(header_items_lookup_20210519_path, encoding='utf-8', lineterminator='\\n')\n",
    "# header_items_lookup_df = header_items_lookup_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# # check if header df and lookup fit\n",
    "# lookup_match_test = header_items_lookup_df[\"headerID\"].nunique() == len(header_items_df)\n",
    "# print(f'#uniue headerIDs in lookup == len(header_df): ',\n",
    "#       f'{lookup_match_test}')\n",
    "\n",
    "# # pre-process descriptions again\n",
    "# header_items_df['description_'] = header_items_df['description_'].astype(str).apply(remove_special_characters)\n",
    "\n",
    "# # clean language tag: replace e.g. {'Deutsch (Untertitel: Deutsch, Englisch)'} -> {'Deutsch'}\n",
    "# header_items_df['language'] = header_items_df['language'].apply(clean_thalia_language_flags)\n",
    "\n",
    "# # clean number_pages\n",
    "# header_items_df['number_pages'] = header_items_df['number_pages'].apply(clean_set)\n",
    "\n",
    "# # drop columns not needed anymore\n",
    "# for col in ['Unnamed: 0', 'recommended_age', 'rec_age']: \n",
    "#     if col in header_items_df.columns:\n",
    "#         header_items_df = header_items_df.drop(columns=col)\n",
    "               \n",
    "display(header_items_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond_missing:  38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headerID</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publisher</th>\n",
       "      <th>mt_st_cl</th>\n",
       "      <th>recommended_age</th>\n",
       "      <th>release_date</th>\n",
       "      <th>mulit_lang_flg</th>\n",
       "      <th>item_lang_en</th>\n",
       "      <th>description_</th>\n",
       "      <th>description_lang</th>\n",
       "      <th>description_en</th>\n",
       "      <th>rec_age</th>\n",
       "      <th>number_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71692</th>\n",
       "      <td>809</td>\n",
       "      <td>a flying birthday cake</td>\n",
       "      <td>louis sachar</td>\n",
       "      <td>bloomsbury publishing plc ,random house</td>\n",
       "      <td>,YFB ,YFQ</td>\n",
       "      <td>6 - 9 Jahr(e)</td>\n",
       "      <td>{Timestamp('1999-09-01 00:00:00'), Timestamp('...</td>\n",
       "      <td>0</td>\n",
       "      <td>English</td>\n",
       "      <td>marvin cannot get to sleep its late the ground...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>small child</td>\n",
       "      <td>{128, 96}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71697</th>\n",
       "      <td>2111</td>\n",
       "      <td>adventures of roopster roux the</td>\n",
       "      <td>lavaille lavette</td>\n",
       "      <td>pelican publishing co</td>\n",
       "      <td>,YFC ,YFB ,YFP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{Timestamp('1998-07-31 00:00:00')}</td>\n",
       "      <td>0</td>\n",
       "      <td>English</td>\n",
       "      <td>with the help of the town librarian roopster r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "      <td>{32}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       headerID                            title            author  \\\n",
       "71692       809           a flying birthday cake      louis sachar   \n",
       "71697      2111  adventures of roopster roux the  lavaille lavette   \n",
       "\n",
       "                                     publisher         mt_st_cl  \\\n",
       "71692  bloomsbury publishing plc ,random house        ,YFB ,YFQ   \n",
       "71697                    pelican publishing co   ,YFC ,YFB ,YFP   \n",
       "\n",
       "      recommended_age                                       release_date  \\\n",
       "71692   6 - 9 Jahr(e)  {Timestamp('1999-09-01 00:00:00'), Timestamp('...   \n",
       "71697             NaN                 {Timestamp('1998-07-31 00:00:00')}   \n",
       "\n",
       "       mulit_lang_flg item_lang_en  \\\n",
       "71692               0      English   \n",
       "71697               0      English   \n",
       "\n",
       "                                            description_ description_lang  \\\n",
       "71692  marvin cannot get to sleep its late the ground...              NaN   \n",
       "71697  with the help of the town librarian roopster r...              NaN   \n",
       "\n",
       "      description_en      rec_age number_pages  \n",
       "71692            NaN  small child    {128, 96}  \n",
       "71697            NaN        adult         {32}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esthe\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "C:\\Users\\esthe\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headerID</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publisher</th>\n",
       "      <th>mt_st_cl</th>\n",
       "      <th>recommended_age</th>\n",
       "      <th>release_date</th>\n",
       "      <th>mulit_lang_flg</th>\n",
       "      <th>item_lang_en</th>\n",
       "      <th>description_</th>\n",
       "      <th>description_lang</th>\n",
       "      <th>description_en</th>\n",
       "      <th>rec_age</th>\n",
       "      <th>number_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71692</th>\n",
       "      <td>809</td>\n",
       "      <td>a flying birthday cake</td>\n",
       "      <td>louis sachar</td>\n",
       "      <td>bloomsbury publishing plc ,random house</td>\n",
       "      <td>,YFB ,YFQ</td>\n",
       "      <td>6 - 9 Jahr(e)</td>\n",
       "      <td>{Timestamp('1999-09-01 00:00:00'), Timestamp('...</td>\n",
       "      <td>0</td>\n",
       "      <td>English</td>\n",
       "      <td>marvin cannot get to sleep its late the ground...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>small child</td>\n",
       "      <td>{128, 96}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71697</th>\n",
       "      <td>2111</td>\n",
       "      <td>adventures of roopster roux the</td>\n",
       "      <td>lavaille lavette</td>\n",
       "      <td>pelican publishing co</td>\n",
       "      <td>,YFC ,YFB ,YFP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{Timestamp('1998-07-31 00:00:00')}</td>\n",
       "      <td>0</td>\n",
       "      <td>English</td>\n",
       "      <td>with the help of the town librarian roopster r...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "      <td>{32}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71699</th>\n",
       "      <td>2874</td>\n",
       "      <td>alpha &amp; omega</td>\n",
       "      <td>markus orths ,hans stengel</td>\n",
       "      <td>schoeffling ,books on demand</td>\n",
       "      <td>,FB ,FLG ,FBA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{Timestamp('2018-09-19 00:00:00'), Timestamp('...</td>\n",
       "      <td>0</td>\n",
       "      <td>German</td>\n",
       "      <td>fuenf nummer 1 alben in folge ein eigenes fitn...</td>\n",
       "      <td>de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "      <td>{256}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71710</th>\n",
       "      <td>5113</td>\n",
       "      <td>ball dont lie</td>\n",
       "      <td>matt de la peña ,matt de la pena</td>\n",
       "      <td>perfection learning corp ,delacorte pr</td>\n",
       "      <td>YXFF ,SFM ,YFR ,YFN ,YFZR</td>\n",
       "      <td>ab 14 Jahr(e)</td>\n",
       "      <td>{Timestamp('2007-03-01 00:00:00')}</td>\n",
       "      <td>0</td>\n",
       "      <td>English</td>\n",
       "      <td>newbery award winning and new york times bests...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>teen</td>\n",
       "      <td>{280}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71713</th>\n",
       "      <td>5456</td>\n",
       "      <td>beastly feast</td>\n",
       "      <td>tracey corderoy</td>\n",
       "      <td>stone arch books ,little tiger press group</td>\n",
       "      <td>,5AG ,YFQ</td>\n",
       "      <td>ab 14 Jahr(e)</td>\n",
       "      <td>{Timestamp('2012-07-01 00:00:00'), Timestamp('...</td>\n",
       "      <td>1</td>\n",
       "      <td>German, English</td>\n",
       "      <td>***ihre beruehrungen sind toedlich doch das ei...</td>\n",
       "      <td>de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>teen</td>\n",
       "      <td>{110}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       headerID                            title  \\\n",
       "71692       809           a flying birthday cake   \n",
       "71697      2111  adventures of roopster roux the   \n",
       "71699      2874                    alpha & omega   \n",
       "71710      5113                    ball dont lie   \n",
       "71713      5456                    beastly feast   \n",
       "\n",
       "                                 author  \\\n",
       "71692                      louis sachar   \n",
       "71697                  lavaille lavette   \n",
       "71699        markus orths ,hans stengel   \n",
       "71710  matt de la peña ,matt de la pena   \n",
       "71713                   tracey corderoy   \n",
       "\n",
       "                                        publisher                   mt_st_cl  \\\n",
       "71692     bloomsbury publishing plc ,random house                  ,YFB ,YFQ   \n",
       "71697                       pelican publishing co             ,YFC ,YFB ,YFP   \n",
       "71699                schoeffling ,books on demand              ,FB ,FLG ,FBA   \n",
       "71710      perfection learning corp ,delacorte pr  YXFF ,SFM ,YFR ,YFN ,YFZR   \n",
       "71713  stone arch books ,little tiger press group                  ,5AG ,YFQ   \n",
       "\n",
       "      recommended_age                                       release_date  \\\n",
       "71692   6 - 9 Jahr(e)  {Timestamp('1999-09-01 00:00:00'), Timestamp('...   \n",
       "71697             NaN                 {Timestamp('1998-07-31 00:00:00')}   \n",
       "71699             NaN  {Timestamp('2018-09-19 00:00:00'), Timestamp('...   \n",
       "71710   ab 14 Jahr(e)                 {Timestamp('2007-03-01 00:00:00')}   \n",
       "71713   ab 14 Jahr(e)  {Timestamp('2012-07-01 00:00:00'), Timestamp('...   \n",
       "\n",
       "       mulit_lang_flg     item_lang_en  \\\n",
       "71692               0          English   \n",
       "71697               0          English   \n",
       "71699               0           German   \n",
       "71710               0          English   \n",
       "71713               1  German, English   \n",
       "\n",
       "                                            description_ description_lang  \\\n",
       "71692  marvin cannot get to sleep its late the ground...               en   \n",
       "71697  with the help of the town librarian roopster r...               en   \n",
       "71699  fuenf nummer 1 alben in folge ein eigenes fitn...               de   \n",
       "71710  newbery award winning and new york times bests...               en   \n",
       "71713  ***ihre beruehrungen sind toedlich doch das ei...               de   \n",
       "\n",
       "      description_en      rec_age number_pages  \n",
       "71692            NaN  small child    {128, 96}  \n",
       "71697            NaN        adult         {32}  \n",
       "71699            NaN        adult        {256}  \n",
       "71710            NaN         teen        {280}  \n",
       "71713            NaN         teen        {110}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exection time langid: 0.3077559471130371 seconds\n",
      "description language exported to: ../data/interim/20210525_header_items_description_lang.csv\n"
     ]
    }
   ],
   "source": [
    "# # find missing items and calculate language\n",
    "# cond_missing = header_items_df['description_'].notna() & header_items_df['description_en'].isna() & header_items_df['description_lang'].isna()\n",
    "# print(f'cond_missing: ', cond_missing.sum())\n",
    "\n",
    "# header_items_df_missing = header_items_df[cond_missing]\n",
    "# display(header_items_df_missing.head(2))\n",
    "\n",
    "# # get start time for performance evaluation\n",
    "# start_time_lg = time.time()\n",
    "\n",
    "# # classify language of descriptions (note: language flag and description language don't necessarily fit !)\n",
    "# # - only choose language of method classify (> ignore confidence score)\n",
    "# header_items_df_missing.loc[:,'description_lang'] = header_items_df_missing['description_'].apply(lambda x: langid.classify(str(x))[0])\n",
    "\n",
    "# # replace lang flg by none if no description given\n",
    "# cond_no_desc = header_items_df_missing['description_'].isna()\n",
    "# header_items_df_missing['description_lang'] = np.where(cond_no_desc, None, header_items_df_missing['description_lang'])\n",
    "# display(header_items_df_missing.head())\n",
    "\n",
    "# # compute execution time\n",
    "# end_time_lg = time.time()\n",
    "# print(f'exection time langid: {end_time_lg - start_time_lg} seconds') # takes ~ 5 mins to compute\n",
    "\n",
    "# # save language flag for description to feather (for simplified reusability)\n",
    "# path = '../data/interim/20210525_header_items_description_lang.csv'\n",
    "# header_items_df_missing.reset_index(inplace=True)\n",
    "# header_items_df_missing.drop(columns='index')\n",
    "# header_items_df_missing[['headerID',\n",
    "#                          'description_',\n",
    "#                          'description_lang']].to_csv(path, index=False)\n",
    "# print(f'description language exported to: {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_items_df_backup = header_items_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond_missing:  0\n",
      "cond_missing:  0\n"
     ]
    }
   ],
   "source": [
    "# # read in missing descriptions\n",
    "# pd.read_csv(path)\n",
    "\n",
    "# # read in translations\n",
    "# missing_desc_de = pd.DataFrame(pd.read_feather('../data/interim/de/transl_de_missing_desc_translations.feather'))\n",
    "# missing_desc_de.drop(columns='index',inplace=True)\n",
    "# missing_desc_es = pd.read_feather('../data/interim/es/transl_es_missing_desc_translations.feather')\n",
    "# missing_desc_es.drop(columns='index',inplace=True)\n",
    "\n",
    "# merge with calculated descriptions\n",
    "\n",
    "# -de\n",
    "# header_items_df_missing = header_items_df_missing.merge(missing_desc_de, on='description_', how='left')\n",
    "# cond_de = header_items_df_missing['description_lang']=='de'\n",
    "# header_items_df_missing['description_en_x'] = np.where(cond_de, header_items_df_missing['description_en_y'],\n",
    "#                                                        header_items_df_missing['description_en_x'])\n",
    "# header_items_df_missing.drop(columns='description_en_y',inplace=True)\n",
    "# header_items_df_missing.rename(columns={'description_en_x': 'description_en'},inplace=True)\n",
    "\n",
    "# # -es\n",
    "# header_items_df_missing = header_items_df_missing.merge(missing_desc_es, on='description_', how='left')\n",
    "# cond_es= header_items_df_missing['description_lang']=='es'\n",
    "# header_items_df_missing['description_en_x'] = np.where(cond_es, header_items_df_missing['description_en_y'],\n",
    "#                                                        header_items_df_missing['description_en_x'])\n",
    "# header_items_df_missing.drop(columns='description_en_y',inplace=True)\n",
    "# header_items_df_missing.rename(columns={'description_en_x': 'description_en'})\n",
    "\n",
    "# # -en\n",
    "# cond_de = header_items_df_missing['description_lang']=='en'\n",
    "# header_items_df_missing['description_en'] = np.where(cond_de, header_items_df_missing['description_'],\n",
    "#                                                        header_items_df_missing['description_en'])\n",
    "\n",
    "# # find missing items \n",
    "# cond_missing = header_items_df_missing['description_'].notna() & header_items_df_missing['description_en'].isna() & header_items_df_missing['description_lang'].isna()\n",
    "# print(f'cond_missing: ', cond_missing.sum())\n",
    "\n",
    "# extract headerIDs of missing items\n",
    "# header_items_missing_lst = list(header_items_df_missing['headerID'])\n",
    "\n",
    "# delete col index from missing items df\n",
    "# header_items_df_missing.drop(columns='index', inplace=True)\n",
    "\n",
    "# delete missing items from headerItemsDF\n",
    "# header_items_df = header_items_df[~header_items_df['headerID'].isin(header_items_missing_lst)]\n",
    "\n",
    "# concat header_items_df and missing items\n",
    "# header_items_df = pd.concat([header_items_df,header_items_df_missing])\n",
    "\n",
    "# # check if still missing items\n",
    "# cond_missing = (header_items_df['description_'].notna()) & (header_items_df['description_en'].isna()) & (header_items_df['description_lang'].isna())\n",
    "# print(f'cond_missing: ', cond_missing.sum())\n",
    "\n",
    "# frequent_lang = ['de', 'es', 'pt', 'it', 'fr']\n",
    "# cond_missing = header_items_df['description_'].notna() & header_items_df['description_en'].isna() & header_items_df['description_lang'].notna() & (header_items_df['description_lang'].isin(frequent_lang))\n",
    "# print(f'cond_missing: ', cond_missing.sum())\n",
    "\n",
    "# # save language flag for description to feather (for simplified reusability)\n",
    "# path = '../data/interim/20210525_header_items_description_lang_2.csv'\n",
    "# header_items_df_missing = header_items_df[cond_missing]\n",
    "# header_items_df_missing.reset_index(inplace=True)\n",
    "# header_items_df_missing.drop(columns='index')\n",
    "# header_items_df_missing[['headerID',\n",
    "#                          'description_',\n",
    "#                          'description_lang']].to_csv(path, index=False)\n",
    "# print(f'description language exported to: {path}')\n",
    "\n",
    "# # read in translations\n",
    "# missing_desc_de = pd.DataFrame(pd.read_feather('../data/interim/de/transl_de_missing_desc_translations_2.feather'))\n",
    "# missing_desc_de.drop(columns='index',inplace=True)\n",
    "\n",
    "# merge with calculated descriptions\n",
    "\n",
    "# -de\n",
    "# header_items_df_missing = header_items_df_missing.merge(missing_desc_de, on='description_', how='left')\n",
    "# cond_de = header_items_df_missing['description_lang']=='de'\n",
    "# header_items_df_missing['description_en_x'] = np.where(cond_de, header_items_df_missing['description_en_y'],\n",
    "#                                                        header_items_df_missing['description_en_x'])\n",
    "# header_items_df_missing.drop(columns='description_en_y',inplace=True)\n",
    "# header_items_df_missing.rename(columns={'description_en_x': 'description_en'},inplace=True)\n",
    "\n",
    "# extract headerIDs of missing items\n",
    "# header_items_missing_lst = list(header_items_df_missing['headerID'])\n",
    "\n",
    "# delete col index from missing items df\n",
    "# header_items_df_missing.drop(columns='index', inplace=True)\n",
    "\n",
    "# delete missing items from headerItemsDF\n",
    "# header_items_df = header_items_df[~header_items_df['headerID'].isin(header_items_missing_lst)]\n",
    "\n",
    "# # concat header_items_df and missing items\n",
    "# header_items_df = pd.concat([header_items_df,header_items_df_missing])\n",
    "# print(header_items_df)\n",
    "\n",
    "# # # check if still missing items\n",
    "# cond_missing = (header_items_df['description_'].notna()) & (header_items_df['description_en'].isna()) & (header_items_df['description_lang'].isna())\n",
    "# print(f'cond_missing: ', cond_missing.sum())\n",
    "\n",
    "# frequent_lang = ['de', 'es', 'pt', 'it', 'fr']\n",
    "# cond_missing = header_items_df['description_'].notna() & header_items_df['description_en'].isna() & header_items_df['description_lang'].notna() & (header_items_df['description_lang'].isin(frequent_lang))\n",
    "# print(f'cond_missing: ', cond_missing.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header_items_df.to_csv('header_items_df_backup.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Backlog] Google API Extract\n",
    "\n",
    "__To do:__\n",
    "1. reduce to one match per item\n",
    "2. include details into items_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Load & Pre-Processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if recompute_gbooks_volumeInfo:\n",
    "    # TODO: insert calculation of volumeInfo\n",
    "else:\n",
    "    volumeInfo_df = pd.read_feather(gbooks_volumeInfo_path_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if load_gbooks_data:\n",
    "    # Load the gbooks details (df)\n",
    "    gbooks_df = pd.read_json(gbooks_path, orient='records')\n",
    "\n",
    "    # reset index (to simplify later join with items_df)\n",
    "    if 'index' in gbooks_df.columns:\n",
    "        gbooks_df = gbooks_df.drop(columns='index')\n",
    "    gbooks_df.reset_index(inplace=True)\n",
    "    gbooks_df = gbooks_df.rename(columns={'index':'itemIdx'})\n",
    "\n",
    "    # get df stats\n",
    "    print(f'gbooks_df:')\n",
    "    display(gbooks_df.head())\n",
    "\n",
    "    print(f'shape gbooks_df: {gbooks_df.shape}')\n",
    "    print(f'shape items_df: {items_df.shape}\\n')\n",
    "\n",
    "    # inspect distribution ot total items\n",
    "    # plt.hist(gbooks_df['totalItems'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if recompute_gbooks_volumeInfo:\n",
    "\n",
    "    batch_start_index = 40000\n",
    "    batch_end_index = len(gbooks_df)\n",
    "    volumeInfo_df = extract_gbook_volumeInfo(gbooks_df.iloc[batch_start_index+1:batch_end_index+1,:],\n",
    "                                            target_keys=['title','publisher','authors','publishedDate','description','printType',\n",
    "                                                           'categories','maturityRating', 'language'])\n",
    "    # inspect head of df\n",
    "    # display(volumeInfo_df.head())\n",
    "\n",
    "    # shape\n",
    "    print(f'shape volumeInfo_df: {volumeInfo_df.shape}\\n')\n",
    "\n",
    "    # get cnt of nas\n",
    "    print(f'na per col: \\n{volumeInfo_df.isna().sum()}\\n')\n",
    "\n",
    "    # value counts specific cols\n",
    "    for col in ['maturityRating', 'printType','language']:\n",
    "        display(pd.DataFrame(volumeInfo_df[col].value_counts()).transpose())\n",
    "        \n",
    "    # exclude magazines\n",
    "    volumeInfo_df = volumeInfo_df.loc[volumeInfo_df['printType']=='BOOK',:]\n",
    "    # volumeInfo_df = volumeInfo_df.drop(columns='printType') #check whether only books in subsequent batches\n",
    "\n",
    "    # reset index before saving as feather\n",
    "    volumeInfo_df.reset_index(inplace=True)\n",
    "    volumeInfo_df = volumeInfo_df.drop(columns='index')\n",
    "    \n",
    "    # save table as feather file (for simplified later load)\n",
    "    gbooks_volumeInfo_path_pp = f'../data/interim/gbooks_volumeInfo_{int(batch_start_index / 1000)}k-{int(batch_end_index / 1000)}k.feather'\n",
    "    volumeInfo_df.to_feather(gbooks_volumeInfo_path_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "test_itemIdx = 9378\n",
    "\n",
    "display(volumeInfo_df.loc[volumeInfo_df['itemIdx']==test_itemIdx,:])\n",
    "display(items_df.iloc[test_itemIdx,:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check all items with just one match\n",
    "gbooks_df.loc[gbooks_df['totalItems'] ==1 ]\n",
    "\n",
    "# compare items_df vs. gbooks_df\n",
    "display(items_df.iloc[2,:])\n",
    "print()\n",
    "print(gbooks_df.iloc[2,0:2])\n",
    "display(gbooks_df.iloc[2,2])\n",
    "\n",
    "# generate subset of gbooks details for testing and further pre-processing\n",
    "gbooks_df_sub = gbooks_df.iloc[0:100,:]\n",
    "display(gbooks_df_sub)\n",
    "\n",
    "# generate json\n",
    "gbooks_json_sub = gbooks_df_sub.to_dict()\n",
    "# gbooks_json_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thalia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if load_thalia_data:\n",
    "\n",
    "    thalia_data_1 = pd.read_pickle('../data/external/thalia_features.pkl')\n",
    "    thalia_data_2 = pd.read_pickle('../data/external/thalia_features_missing_items.pkl')\n",
    "    thalia_data = pd.concat([thalia_data_1, thalia_data_2])\n",
    "    \n",
    "    print('thalia_data: ', thalia_data.shape)\n",
    "    print(thalia_data.head(5))\n",
    "\n",
    "if merge_thalia_data_items_df:\n",
    "    \n",
    "    print('items_df: ',items_df.shape)\n",
    "    items_df = items_df.merge(thalia_data, on='itemID', how='left')\n",
    "    print('items_df: ',items_df.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load thalia data for missing items\n",
    "thalia_data = pd.read_pickle('../data/external/thalia_features_missing_items.pkl')\n",
    "display(thalia_data)\n",
    "\n",
    "items_df = items_df.merge(thalia_data, on='itemID', how='left')\n",
    "display(items_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check if missing items in thalia data\n",
    "items_df.merge(thalia_data, on='itemID', how='left')['cover_url'].isna().sum() # items also not in thalia data !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "- only for __transactions__: remove transactions with suspiciously high #of clicks/basket/order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Original shape:', transactions_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(transactions_df['click'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "transactions_df = transactions_df[transactions_df['click'] < np.quantile(transactions_df.click, 0.99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(transactions_df['basket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "transactions_df = transactions_df[transactions_df['basket'] < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(transactions_df['order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "transactions_df = transactions_df[transactions_df['order'] < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('After outlier removal shape:', transactions_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String normalization\n",
    "\n",
    "__Applied:__\n",
    "1. conversion to lowercase, e.g. publisher = 'TEKTIME' or 'Tektime' to 'tektime'\n",
    "2. removal of leading special characters, e.g. \",william shakespeare\"\n",
    "3. conversion of unicode characters (ä,ö,ü)\n",
    "\n",
    "__No fix yet:__\n",
    "1. weird entries\n",
    "    - author: der Authhhhor\n",
    "    - diverse Autoren, Autoren\n",
    "3. unicode characters like (à,é,è,°o)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# generate copy of original df for testing of pre-processing\n",
    "items_df_cl = items_df.copy()\n",
    "display(items_df_cl.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cols_pp = ['title', 'author', 'publisher']\n",
    "\n",
    "# convert all strings to lowercase\n",
    "items_df[cols_pp] = items_df[cols_pp].applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "for col in cols_pp:\n",
    "    \n",
    "    col_cl = col + '_cl'\n",
    "\n",
    "    # add additional col for pp titles\n",
    "    items_df[col_cl] = items_df[col]\n",
    "\n",
    "    # clean strings\n",
    "    if col == 'title':\n",
    "        items_df[col_cl] = items_df[col_cl].apply(remove_nontitle_substrings)\n",
    "    items_df[col_cl] = items_df[col_cl].astype(str).apply(remove_special_characters)\n",
    "    items_df[col_cl] = items_df[col_cl].apply(convert_umlaute)\n",
    "\n",
    "    # reduce all spaces in the articles to single spaces\n",
    "    items_df[col_cl] = items_df[col_cl].apply(remove_duplicate_whitespace)\n",
    "\n",
    "    # print stats\n",
    "    col_cnt_unique = items_df[col].nunique()\n",
    "    col_cl_cnt_unique = items_df[col_cl].nunique()\n",
    "    print(f'# unique {col} (before preprocessing): {col_cnt_unique} / {len(items_df)}')\n",
    "    print(f'# unique {col} (after preprocessing): {col_cnt_unique} / {len(items_df)}')\n",
    "    print(f'# reduction in unique {col}: {col_cnt_unique-col_cl_cnt_unique}\\n')\n",
    "    \n",
    "# replace original cols by pre-processed cols\n",
    "items_df = items_df.drop(columns=cols_pp)\n",
    "items_df = items_df.rename(columns={'title_cl': 'title', 'author_cl': 'author', 'publisher_cl': 'publisher'})\n",
    "\n",
    "# remove items with missing title after pre-processing\n",
    "print(f\"remove items with missing/empty title after pp: {(items_df['title']=='').sum()}\")\n",
    "items_df = items_df[items_df['title']!='']\n",
    "\n",
    "# display cleaned df head\n",
    "display(items_df.head(10))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check if fit in existing header_df\n",
    "missing_items_header_fit =  items_df_missing.merge(header_items_df, on='title', how='left')\n",
    "print(f'cnt items that could be matched to existing header item: ',\n",
    "      f'{missing_items_header_fit[\"headerID\"].notna().sum()}/{len(missing_items_header_fit)}')\n",
    "\n",
    "# # check if data available for missing items in evaluation_df\n",
    "# evaluation_header_df = header_items_lookup_df.merge(evaluation_df, on='itemID',how='right')\n",
    "# evaluation_header_df[evaluation_header_df['headerID'].isna()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# missing_items_header_fit[missing_items_header_fit[\"headerID\"].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### title"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "# generate titles df (with comparison column for original and cleaned title)\n",
    "titles_df = pd.DataFrame(items_df_cl[\"title\"].unique()).rename(columns={0: \"title\"})\n",
    "titles_df['title_cl'] = titles_df['title']\n",
    "\n",
    "# convert all strings to lowercase\n",
    "titles_df = titles_df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "# clean strings\n",
    "titles_df['title_cl'] = titles_df['title_cl'].astype(str).apply(remove_special_characters)\n",
    "titles_df['title_cl'] = titles_df['title_cl'].apply(remove_nontitle_substrings)\n",
    "titles_df['title_cl'] = titles_df['title_cl'].apply(convert_umlaute)\n",
    "\n",
    "# reduce all spaces in the articles to single spaces\n",
    "titles_df['title_cl'] = titles_df['title_cl'].apply(remove_duplicate_whitespace)\n",
    "\n",
    "# print stats\n",
    "title_cnt_unique = titles_df[\"title\"].nunique()\n",
    "title_cl_cnt_unique = titles_df[\"title_cl\"].nunique()\n",
    "print(f'# unique titles (before preprocessing): {title_cnt_unique} / {len(titles_df)}')\n",
    "print(f'# unique titles (after preprocessing): {title_cl_cnt_unique} / {len(titles_df)}')\n",
    "print(f'# reduction in unique titles: {title_cnt_unique-title_cl_cnt_unique}')\n",
    "\n",
    "# display cleaned df head\n",
    "display(titles_df.head(10))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "test_itemIdx = 3592\n",
    "\n",
    "display(volumeInfo_df.loc[volumeInfo_df['itemIdx']==test_itemIdx,:])\n",
    "display(items_df.iloc[test_itemIdx,:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Testing of removal    \n",
    "for book in ['unsterblich 02 - tor der nacht','meine kindergarten-freunde (pferde)']:\n",
    "    book = re.sub(r'-',' ',book)\n",
    "    print(book)\n",
    "\n",
    "# print cnt of items including special terms\n",
    "print(f'#items with title including:')\n",
    "col = \"title_cl\"\n",
    "for entry in ['hardcover','taschenbuch','edition','novel','hardback']:\n",
    "    cnt = titles_df[col].str.contains(f'{entry}').sum()\n",
    "    print(f'\\t{entry}: {cnt}')\n",
    "\n",
    "# search for specific entry\n",
    "#pd.set_option(\"display.max_rows\", None)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#search_entry = r' +'\n",
    "#display(titles_df.loc[titles_df['title'].str.contains(f'{search_entry}'), :])\n",
    "\n",
    "# inspect matches for specific terms/patterns\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "# p = re.compile('\\(.*\\)')\n",
    "p = re.compile(r'edition')\n",
    "col = \"title_cl\"\n",
    "matches = titles_df[col].apply(lambda s: p.findall(s))\n",
    "matches = pd.DataFrame(set(flatten([x for x in matches if x])))\n",
    "display(matches)\n",
    "\n",
    "# cnt unique items per title\n",
    "title_cnt_bpp = titles_df.groupby('title').count().reset_index().rename(columns={'title_cl': 'cnt'})\n",
    "title_cnt_app = titles_df.groupby('title_cl').count().reset_index().rename(columns={'title': 'cnt'})\n",
    "# display(title_cnt_bpp)\n",
    "# display(title_cnt_app)\n",
    "\n",
    "# merge both cnts to get comparison\n",
    "titles_w_cnt = titles_df.merge(title_cnt_bpp, on='title', how='left')\n",
    "titles_w_cnt = titles_w_cnt.merge(title_cnt_app, on='title_cl', how='left')\n",
    "# display(titles_w_cnt)\n",
    "\n",
    "# inspect differences\n",
    "print(f'items with additional title matches: {len(titles_w_cnt[(titles_w_cnt[\"cnt_x\"] < titles_w_cnt[\"cnt_y\"])].drop_duplicates())}')\n",
    "\n",
    "display(titles_w_cnt[(titles_w_cnt['cnt_x'] < titles_w_cnt['cnt_y']) & \n",
    "                     (titles_w_cnt['cnt_y'] > 1)].drop_duplicates())\n",
    "                     \n",
    "# inspect exemplary item\n",
    "titles_df[titles_df['title_cl'] == 'the dungeon masters wife']\n",
    "titles_df[titles_df['title_cl'] == 'z rex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### author"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# generate authors df (with comparison column for original and cleaned author)\n",
    "author_df = pd.DataFrame(items_df_cl[\"author\"].unique()).rename(columns={0: \"author\"})\n",
    "author_df['author_cl'] = author_df['author']\n",
    "\n",
    "# convert all strings to lowercase\n",
    "author_df = author_df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "# clean strings\n",
    "author_df['author_cl'] = author_df['author_cl'].astype(str).apply(remove_special_characters)\n",
    "author_df['author_cl'] = author_df['author_cl'].apply(convert_umlaute)\n",
    "\n",
    "# reduce all spaces in the articles to single spaces\n",
    "author_df['author_cl'] = author_df['author_cl'].apply(remove_duplicate_whitespace)\n",
    "\n",
    "# print stats\n",
    "author_cnt_unique = author_df[\"author\"].nunique()\n",
    "author_cl_cnt_unique = author_df[\"author_cl\"].nunique()\n",
    "print(f'# unique authors (before preprocessing): {author_cnt_unique} / {len(author_df)}')\n",
    "print(f'# unique authors (after preprocessing): {author_cl_cnt_unique} / {len(author_df)}')\n",
    "print(f'# reduction in unique authors: {author_cnt_unique-author_cl_cnt_unique}')\n",
    "\n",
    "# display cleaned df head\n",
    "display(author_df.head(10))\n",
    "\n",
    "# cnt unique items per author\n",
    "author_cnt_bpp = author_df.groupby('author').count().reset_index().rename(columns={'author_cl': 'cnt'})\n",
    "author_cnt_app = author_df.groupby('author_cl').count().reset_index().rename(columns={'author': 'cnt'})\n",
    "# display(author_cnt_bpp)\n",
    "# display(author_cnt_app)\n",
    "\n",
    "# merge both cnts to get comparison\n",
    "authors_w_cnt = author_df.merge(author_cnt_bpp, on='author', how='left')\n",
    "authors_w_cnt = authors_w_cnt.merge(author_cnt_app, on='author_cl', how='left')\n",
    "# display(authors_w_cnt)\n",
    "\n",
    "# inspect differences\n",
    "print(f'items with additional author matches: {len(authors_w_cnt[(authors_w_cnt[\"cnt_x\"] < authors_w_cnt[\"cnt_y\"])].drop_duplicates())}')\n",
    "\n",
    "display(authors_w_cnt[(authors_w_cnt['cnt_x'] < authors_w_cnt['cnt_y']) & \n",
    "                     (authors_w_cnt['cnt_y'] > 1)].drop_duplicates())\n",
    "\n",
    "# inspect exemplary item\n",
    "author_df[author_df['author_cl'] == 'larry w miller jr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### publisher"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# generate publishers df (with comparison column for original and cleaned publisher)\n",
    "publisher_df = pd.DataFrame(items_df_cl[\"publisher\"].unique()).rename(columns={0: \"publisher\"})\n",
    "publisher_df['publisher_cl'] = publisher_df['publisher']\n",
    "\n",
    "# convert all strings to lowercase\n",
    "publisher_df = publisher_df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "# clean strings\n",
    "publisher_df['publisher_cl'] = publisher_df['publisher_cl'].astype(str).apply(remove_special_characters)\n",
    "publisher_df['publisher_cl'] = publisher_df['publisher_cl'].apply(convert_umlaute)\n",
    "\n",
    "# reduce all spaces in the articles to single spaces\n",
    "publisher_df['publisher_cl'] = publisher_df['publisher_cl'].apply(remove_duplicate_whitespace)\n",
    "\n",
    "# print stats\n",
    "publisher_cnt_unique = publisher_df[\"publisher\"].nunique()\n",
    "publisher_cl_cnt_unique = publisher_df[\"publisher_cl\"].nunique()\n",
    "print(f'# unique publishers (before preprocessing): {publisher_cnt_unique} / {len(publisher_df)}')\n",
    "print(f'# unique publishers (after preprocessing): {publisher_cl_cnt_unique} / {len(publisher_df)}')\n",
    "print(f'# reduction in unique publishers: {publisher_cnt_unique-publisher_cl_cnt_unique}')\n",
    "\n",
    "# display cleaned df head\n",
    "display(publisher_df.head(10))\n",
    "\n",
    "# cnt unique items per publisher\n",
    "publisher_cnt_bpp = publisher_df.groupby('publisher').count().reset_index().rename(columns={'publisher_cl': 'cnt'})\n",
    "publisher_cnt_app = publisher_df.groupby('publisher_cl').count().reset_index().rename(columns={'publisher': 'cnt'})\n",
    "# display(publisher_cnt_bpp)\n",
    "# display(publisher_cnt_app)\n",
    "\n",
    "# merge both cnts to get comparison\n",
    "publishers_w_cnt = publisher_df.merge(publisher_cnt_bpp, on='publisher', how='left')\n",
    "publishers_w_cnt = publishers_w_cnt.merge(publisher_cnt_app, on='publisher_cl', how='left')\n",
    "# display(publishers_w_cnt)\n",
    "\n",
    "# inspect differences\n",
    "print(f'items with additional publisher matches: {len(publishers_w_cnt[(publishers_w_cnt[\"cnt_x\"] < publishers_w_cnt[\"cnt_y\"])].drop_duplicates())}')\n",
    "\n",
    "display(publishers_w_cnt[(publishers_w_cnt['cnt_x'] < publishers_w_cnt['cnt_y']) & \n",
    "                     (publishers_w_cnt['cnt_y'] > 1)].drop_duplicates())\n",
    "\n",
    "# inspect exemplary item\n",
    "publisher_df[publisher_df['publisher_cl'] == 'digital scanning inc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header-Set \n",
    "\n",
    "__Approach:__\n",
    "1. __[done]__ Generate new header-set with new IDs to unify same books that appear multiple times in the items and transactions table\n",
    "    a. generate new IDs\n",
    "    b. unify information\n",
    "2. __[done]__ Replace the subset IDs in transactions table by superset IDs\n",
    "\n",
    "3. __[done]__ Pull data on header level from external sources (so far: only thalia data - pulled on item_df level and then aggregated on header_items_df level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if recompute_header_set:\n",
    "\n",
    "    # generate header set with unique ids for \"super-items\"\n",
    "    header_items_df = generate_header_set(items_df)\n",
    "    \n",
    "    # add headerID to items_df (drop before join if already existent)\n",
    "    if 'headerID' in items_df.columns:\n",
    "        items_df = items_df.drop(columns=['headerID'])\n",
    "    items_df = items_df.merge(header_items_df[['title','headerID']], left_on='title', right_on='title',how='left') \n",
    "    display(items_df.head())\n",
    "    print(f'missing headerIDs in items_df: {items_df[\"headerID\"].isnull().sum()}')\n",
    "\n",
    "    # generate lookup table\n",
    "    header_items_lookup_df = items_df[['itemID','headerID']].drop_duplicates()\n",
    "    print(f'shape of items_df vs. header_items_lookup_df: {items_df.shape} vs. {header_items_lookup_df.shape}')\n",
    "    \n",
    "    # clean language tag: replace e.g. {'Deutsch (Untertitel: Deutsch, Englisch)'} -> {'Deutsch'}\n",
    "    header_items_df['language'] = header_items_df['language'].apply(lambda x: str(x)).apply(clean_thalia_language_flags)\n",
    "\n",
    "    # clean number_pages\n",
    "    header_items_df['number_pages'] = header_items_df['number_pages'].apply(lambda x: str(x)).apply(clean_set)\n",
    "    \n",
    "    display(header_items_df.head(5))\n",
    "    \n",
    "#     # add headerID to transactions_df (drop before join if already existent)\n",
    "#     if 'headerID' in transactions_df.columns:\n",
    "#         transactions_df = transactions_df.drop(columns=['headerID'])\n",
    "#     transactions_df = transactions_df.merge(header_items_lookup_df, left_on='itemID', right_on='itemID',how='left') \n",
    "\n",
    "#     # inspect results\n",
    "#     display(transactions_df.head())\n",
    "#     print(f'# missing headerIDs in transactions_df: {transactions_df[\"headerID\"].isnull().sum()}')\n",
    "#     print(f'# unique items in transactions_df: {transactions_df[\"itemID\"].nunique()}')\n",
    "#     print(f'# unique headers in transactions_df: {transactions_df[\"headerID\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Backlog] merge with Google data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if load_ppgbooks_data_and_merge_with_header:\n",
    "\n",
    "    # read in preprocessed data\n",
    "    g_data = pd.read_feather('../data/processed/gbooks_volumeInfo_pp.feather')\n",
    "    g_data.head(1)\n",
    "    \n",
    "    cols_pp = ['title', 'authors', 'publisher']\n",
    "\n",
    "    # convert all strings to lowercase\n",
    "    g_data[cols_pp] = g_data[cols_pp].applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "    \n",
    "    header_items_df.loc[:, 'author_'] = header_items_df.author.map(lambda x: next(iter(x)))\n",
    "    header_items_df.loc[:, 'publisher_'] = header_items_df.publisher.map(lambda x: next(iter(x)))\n",
    "    header_items_df.loc[:, 'release_date_'] = header_items_df.release_date.map(lambda x: next(iter(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if load_ppgbooks_data_and_merge_with_header:\n",
    "    for col in cols_pp:\n",
    "\n",
    "        col_cl = col + '_cl'\n",
    "\n",
    "        # add additional col for pp titles\n",
    "        g_data[col_cl] = g_data[col]\n",
    "\n",
    "        # clean strings\n",
    "        if col == 'title':\n",
    "            g_data[col_cl] = g_data[col_cl].apply(remove_nontitle_substrings)\n",
    "        g_data[col_cl] = g_data[col_cl].astype(str).apply(remove_special_characters)\n",
    "        g_data[col_cl] = g_data[col_cl].apply(convert_umlaute)\n",
    "\n",
    "        # reduce all spaces in the articles to single spaces\n",
    "        g_data[col_cl] = g_data[col_cl].apply(remove_duplicate_whitespace)\n",
    "\n",
    "        # print stats\n",
    "        #col_cnt_unique = g_data[col].nunique()\n",
    "        #col_cl_cnt_unique = g_data[col_cl].nunique()\n",
    "        #print(f'# unique {col} (before preprocessing): {col_cnt_unique} / {len(items_df)}')\n",
    "        #print(f'# unique {col} (after preprocessing): {col_cnt_unique} / {len(items_df)}')\n",
    "        #print(f'# reduction in unique {col}: {col_cnt_unique-col_cl_cnt_unique}\\n')\n",
    "\n",
    "    # replace original cols by pre-processed cols\n",
    "    g_data = g_data.drop(columns=cols_pp)\n",
    "    g_data = g_data.rename(columns={'title_cl': 'title', 'author_cl': 'author', 'publisher_cl': 'publisher'})\n",
    "\n",
    "    # remove items with missing title after pre-processing\n",
    "    print(f\"remove items with missing/empty title after pp: {(g_data['title']=='').sum()}\")\n",
    "    g_data = g_data[g_data['title']!='']\n",
    "\n",
    "    # display cleaned df head\n",
    "    display(g_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#pd.merge(header_items_df, g_data[['title', 'publisher','publishedDate', 'language', 'maturityRating']], left_on=['title','release_date_'], right_on=['title', 'publishedDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language flag\n",
    "\n",
    "__Idea:__\n",
    "Flag Language of title in order to improve same language recommendations\n",
    "\n",
    "__Lookup Links:__\n",
    "1. [stackoverflow:](https://stackoverflow.com/questions/39142778/python-how-to-determine-the-language) comparison of different language detection modules\n",
    "2. [tds](https://towardsdatascience.com/benchmarking-language-detection-for-nlp-8250ea8b67c) performance evaluation -> recommends __fasttext__\n",
    "\n",
    "__Note on fasttext:__\n",
    "- official Python binding module by Facebook\n",
    "- problems with installation on windows"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# define test strings\n",
    "str_en = \"romeo and juliet: the graphic novel\"\n",
    "str_de = \"sternenschweif. zauberhafter schulanfang\"\n",
    "\n",
    "# define whether to use existing flags and df\n",
    "if not recompute_lg_flg:\n",
    "    items_df = items_df_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### module testing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "if recompute_lg_flg:\n",
    "    # module detector dict\n",
    "    lan_detector = {'ld': 'langdetect', 'gl': 'guess_language', 'lg': 'langid'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### langdetect (=title_ld)\n",
    "[langdetect](https://pypi.org/project/langdetect/)\n",
    "- important: use try-catch block to handle e.g. numerics, urls etc\n",
    "- non-deterministic approach: remember to set seed for reproducible results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "if recompute_lg_flg:\n",
    "    from langdetect import DetectorFactory, detect\n",
    "    from langdetect.lang_detect_exception import LangDetectException"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "# test detector on sample strings\n",
    "print(detect(str_en))\n",
    "print(detect(str_de))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "if recompute_lg_flg:\n",
    "    # get start time for performance evaluation\n",
    "    start_time_ld = time.time()\n",
    "\n",
    "    # set seed for reproducability\n",
    "    DetectorFactory.seed = 0\n",
    "\n",
    "    # option 1: pre-calculate list of languages\n",
    "    title_ld = []\n",
    "    for title in items_df['title']:\n",
    "        try:\n",
    "            title_ld.append(detect(title))\n",
    "    #         print(f'{title}: {detect(title)}')\n",
    "        except LangDetectException:\n",
    "            title_ld.append(None)\n",
    "    #         print(f'{title}: \"undefined\"')\n",
    "\n",
    "    # compute execution time\n",
    "    end_time_ld = time.time()\n",
    "    print(f'exection time langdetect: {end_time_ld - start_time_ld} seconds')\n",
    "\n",
    "    items_df['title_ld'] = title_ld\n",
    "\n",
    "    # option 2: use apply and title col\n",
    "    # items_df['title_ld'] = items_df['title'].apply(lambda x: detect(x) if not x.isnumeric() else None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "# inspect items w/o language specification -> only numeric !\n",
    "print(f'cnt of items without language flag: {items_df[\"title_ld\"].isnull().sum()}')\n",
    "display(items_df[items_df[\"title_ld\"].isnull()].head(10))\n",
    "\n",
    "# inspect results\n",
    "ld_vc = pd.DataFrame(items_df['title_ld'].value_counts().reset_index())\n",
    "display(ld_vc.transpose())\n",
    "\n",
    "# show barplot with # items with title in given language\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.barplot(x='index', y='title_ld', ax=ax, data=ld_vc, palette=palette_blue).set(\n",
    "    xlabel='languages determined by \"langdetect\"',\n",
    "    ylabel='# items with title in given language'\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### guess_language (=title_gl)\n",
    "\n",
    "- Can detect very short samples"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "if recompute_lg_flg:\n",
    "    from guess_language import guess_language"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(guess_language(str_en))\n",
    "print(guess_language(str_de))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "if recompute_lg_flg:\n",
    "\n",
    "    # get start time for performance evaluation\n",
    "    start_time_gl = time.time()\n",
    "\n",
    "    # detect langauge of titles\n",
    "    items_df['title_gl'] = items_df['title'].apply(lambda x: guess_language(x) if not x.isnumeric() else None)\n",
    "\n",
    "    # set 'UNKNOWN' to None\n",
    "    items_df.loc[items_df['title_gl']=='UNKNOWN','title_gl'] = None\n",
    "\n",
    "    # compute execution time\n",
    "    end_time_gl = time.time()\n",
    "    print(f'exection time guess_language: {end_time_gl - start_time_gl} seconds')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "# inspect results\n",
    "gl_vc = pd.DataFrame(items_df['title_gl'].value_counts().reset_index())\n",
    "display(gl_vc.transpose())\n",
    "\n",
    "# show barplot with # items with title in given language\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.barplot(x='index', y='title_gl', ax=ax, data=gl_vc, palette=palette_blue).set(\n",
    "    xlabel='languages determined by \"guess_language\"',\n",
    "    ylabel='# items with title in given language'\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### langid (=title_lg)\n",
    "\n",
    "- @param instance a text string. Unicode strings will automatically be utf8-encoded \n",
    "- @returns a tuple of the most likely language and the confidence score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "if recompute_lg_flg:\n",
    "    import langid"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "langid.classify(str_en)\n",
    "langid.classify(str_de)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "if recompute_lg_flg:\n",
    "\n",
    "    # get start time for performance evaluation\n",
    "    start_time_lg = time.time()\n",
    "\n",
    "    # option 1: pre-calculate list of languages\n",
    "    title_lg = []\n",
    "\n",
    "    for title in items_df['title']:\n",
    "        title_lg.append(langid.classify(title))\n",
    "        print(f'{title}: {langid.classify(title)}')\n",
    "\n",
    "    # compute execution time\n",
    "    end_time_lg = time.time()\n",
    "    print(f'exection time langid: {end_time_lg - start_time_lg} seconds')\n",
    "\n",
    "    # add col to df\n",
    "    items_df['title_lg'] = [t[0] for t in title_lg]\n",
    "\n",
    "    # option 2: use apply\n",
    "    # items_df['title_lg'] = items_df['title'].apply(lambda x: TextBlob(x).detect_language() if not x.isnumeric() or  else None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "# inspect items w/o language specification -> only numeric !\n",
    "print(f'cnt of items without language flag: {items_df[\"title_lg\"].isnull().sum()}')\n",
    "#display(items_df[items_df[\"title_lg\"].isnull()].head(10))\n",
    "\n",
    "# inspect results\n",
    "lg_vc = pd.DataFrame(items_df['title_lg'].value_counts().reset_index())\n",
    "display(lg_vc.transpose())\n",
    "\n",
    "# show barplot with # items with title in given language\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.barplot(x='index', y='title_lg', ax=ax, data=lg_vc, palette=palette_blue).set(\n",
    "    xlabel='languages determined by \"langid\"',\n",
    "    ylabel='# items with title in given language'\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### module performance evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# compare execution time and items w/o flag\n",
    "if recompute_lg_flg:\n",
    "    lan_detector_eval_df = pd.DataFrame({'execution time [s]': [eval('end_time_'+det.split(\"_\")[1]) - eval('start_time_'+det.split(\"_\")[1]) for det in ['title_ld','title_gl','title_lg']],\n",
    "                                        '#items w/o language flg':[items_df[det].isnull().sum() for det in ['title_ld','title_gl','title_lg']]},\n",
    "                                       index=[det for det in lan_detector.values()])\n",
    "    display(lan_detector_eval_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# merge results dfs\n",
    "ld_gl_vc = ld_vc.merge(gl_vc, left_on='index', right_on='index', how='outer')\n",
    "ld_gl_lg_vc = ld_gl_vc.merge(lg_vc, left_on='index', right_on='index', how='outer')\n",
    "display(ld_gl_lg_vc.transpose())\n",
    "ld_gl_lg_vc = ld_gl_lg_vc.head(10)\n",
    "\n",
    "# rename columns\n",
    "ld_gl_lg_vc.columns = ['index', 'langdetect','guess_language','langid']\n",
    "\n",
    "# add language name\n",
    "ld_gl_lg_vc['language_name'] = ld_gl_lg_vc['index'].apply(lambda l: pycountry.countries.get(alpha_2=l).name if l != 'en' else 'English')\n",
    "\n",
    "# transform model cols into identifier column for plotting\n",
    "ld_gl_lg_vc = pd.melt(ld_gl_lg_vc, id_vars=[\"index\", \"language_name\"],\n",
    "                  var_name=\"flag_m\", value_name=\"idCnt\")\n",
    "#display(ld_gl_lg_vc)\n",
    "\n",
    "# Draw a nested barplot by language detector\n",
    "sns.set_theme()\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "g = sns.barplot(y=\"language_name\", x=\"idCnt\", hue=\"flag_m\", data=ld_gl_lg_vc, palette=palette_blue, orient='h')\n",
    "g.set(xlabel=\"# itemID\", ylabel = \"\")\n",
    "g.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DEV] Topic Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# creation of sentence embeddings for each category\n",
    "\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load embeddings\n",
    "model = KeyedVectors.load_word2vec_format('../tempData/fetchedData/fasttext.wiki.en.300.vocab_200K.vec')\n",
    "df = pd.read_json('../tempData/processedData/categories_translated.json', typ='series')\n",
    "\n",
    "# create cat strings\n",
    "def cats(row):\n",
    "    words = ''\n",
    "    cat_ins = ''\n",
    "    for char in row:\n",
    "        cat_ins = cat_ins + char\n",
    "        try:\n",
    "            words = words + df.loc[cat_ins].lower() +' '\n",
    "        except:\n",
    "            pass\n",
    "    return words\n",
    "\n",
    "# calculate averaged sentence embedding\n",
    "def calc(words):\n",
    "    summ = 0\n",
    "    i = 0\n",
    "    for word in set(words.split()):\n",
    "        try:\n",
    "            summ = summ + model[word]\n",
    "            i = i+1\n",
    "        except:\n",
    "            pass\n",
    "    return summ/i\n",
    "\n",
    "\n",
    "def f(col):\n",
    "    s=0\n",
    "    try:\n",
    "        for e in col:\n",
    "            s = s + embs.loc[e][0]\n",
    "        return s / len(col)\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# loop trough all cats & save as list to be able to save in df\n",
    "avgs = []\n",
    "for i in df.index:\n",
    "    avgs.append([calc(cats(i))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "embs = pd.DataFrame(avgs, index=df.index, columns=['fasttext_emb'])\n",
    "# embs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df.mt_st_cl = header_items_df.mt_st_cl.str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# create list from set\n",
    "#header_items_df.loc[:, 'mt_st'] = header_items_df.mt_st_cl.map(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df.loc[:, 'mt_st'] = header_items_df.mt_st_cl.map(lambda x: ' '.join(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df.drop('mt_st_cl', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# add categorical embeddings\n",
    "header_items_df['emb_cats'] = header_items_df.apply(lambda x: f(x['mt_st']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thalia Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thalia Language Flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate: flag for multi-lingual books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # test how many entries in set\n",
    "# print(header_items_df['language'].apply(lambda x: len(eval(x))).value_counts(dropna=False))\n",
    "\n",
    "# note: \n",
    "# - 1 means only one item in set (=but can still be multiple langs separated by comma)\n",
    "# - 2 means two itmes in set (=generated due to header aggregation)\n",
    "\n",
    "# generate multi-lingual flag (based on splitting by comma > both due to header agg AND given in Thalia description)\n",
    "cond_multi_lang = header_items_df['language'].apply(lambda x: len(x.split(','))) > 1\n",
    "header_items_df['mulit_lang_flg'] = np.where(cond_multi_lang,1,0)\n",
    "\n",
    "# print value counts for both flags > only use mulit_lang_flg (only 84 items might be mis-classified)\n",
    "display(header_items_df[['mulit_lang_flg']].value_counts())\n",
    "\n",
    "# inspect:\n",
    "display(header_items_df[header_items_df['mulit_lang_flg'] == 1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test how many entries in set\n",
    "print(header_items_df['language'].apply(lambda x: len(eval(x)).value_counts(dropna=False))\n",
    "\n",
    "# note: \n",
    "# - 1 means only one item in set (=but can still be multiple langs separated by comma)\n",
    "# - 2 means two itmes in set (=generated due to header aggregation)\n",
    "\n",
    "# generate multi-lingual flag (based on splitting by comma > both due to header agg AND given in Thalia description)\n",
    "cond_multi_lang = header_items_df['language'].apply(lambda x: len(x.split(','))) > 1\n",
    "header_items_df['mulit_lang_flg'] = np.where(cond_multi_lang,1,0)\n",
    "\n",
    "# print value counts for both flags > only use mulit_lang_flg (only 84 items might be mis-classified)\n",
    "display(header_items_df[['mulit_lang_flg']].value_counts())\n",
    "\n",
    "# inspect:\n",
    "display(header_items_df[header_items_df['mulit_lang_flg'] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-Process: lang column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# clean lang column\n",
    "header_items_df['lang'] = header_items_df['language'].apply(\n",
    "    lambda x: ', '.join(sorted(list(set(unify_set_from_str(x).split(',')))))\n",
    ")\n",
    "\n",
    "# replace \"{''}\" entries with None\n",
    "cond_replace_None = (header_items_df['language'] == \"{''}\")\n",
    "header_items_df['lang'] = np.where(cond_replace_None, None, header_items_df['lang'])\n",
    "\n",
    "# rename lang column to item_lang_de\n",
    "header_items_df = header_items_df.rename(columns={'lang': 'item_lang_de'})\n",
    "display(header_items_df.head(2))\n",
    "\n",
    "# inspect cases with multiple languages (!): \n",
    "# (a) book is in multiple languages: {'Deutsch, Italienisch'} or {'Deutsch, Englisch (Untertitel: Englisch)'}\n",
    "# (b) two languages due to merge: {'Deutsch', 'Italienisch'}\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "for lan in ['language','item_lang_de']:\n",
    "    \n",
    "    print(f'******************************************')\n",
    "    lan_dist = pd.DataFrame(header_items_df[lan].value_counts().reset_index())\n",
    "    lan_dist_sub = lan_dist[lan_dist[lan]>4]\n",
    "    print(f'#distinct language combos in \"{lan}\": {len(lan_dist_sub)}')\n",
    "    display(lan_dist_sub)\n",
    "    sns.barplot(y='index',x=lan, data=lan_dist_sub, orient='h')\n",
    "    plt.show()\n",
    "    \n",
    "pd.set_option(\"display.max_rows\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__inspect pre-processing results__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test entries with len(set) == 1 but multi-lingual\n",
    "# cond = header_items_df['language'] == \"{'Arabisch, Englisch'}\"\n",
    "cond_multi_lang = header_items_df['language'].apply(lambda x: len(x.split(','))) > 1\n",
    "header_items_df['mulit_lang_flg'] = np.where(cond_multi_lang,1,0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test entries with two languages in set (generated due to header aggregation)\n",
    "header_items_df[header_items_df['language'].apply(lambda x: len(eval(x))) == 2].head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Translate: language flag"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# specify the model name\n",
    "model_name = f'Helsinki-NLP/opus-mt-de-en'\n",
    "print(f'model used: {model_name}')   \n",
    "\n",
    "# Download the model and the tokenizer\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def translate_language_flg(lang_flag_):\n",
    "\n",
    "    # tokenize description text\n",
    "    inputs = tokenizer(lang_flag_, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # apply model and translate tokens\n",
    "    gen = model.generate(**inputs)\n",
    "    translation = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "    \n",
    "    return translation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# generate df with unique languages (to reduce dim of translations)\n",
    "header_items_lang = pd.DataFrame(header_items_df['item_lang_de'].drop_duplicates())\n",
    "header_items_lang = header_items_lang[header_items_lang['item_lang_de'].notna()]\n",
    "\n",
    "# translate language flags\n",
    "header_items_lang['item_lang_en'] = header_items_lang['item_lang_de'].apply(translate_language_flg)\n",
    "\n",
    "# convert list to str\n",
    "header_items_lang['item_lang_en'] = header_items_lang['item_lang_en'].apply(lambda x: ''.join(x))\n",
    "\n",
    "# reset index \n",
    "header_items_lang =header_items_lang.reset_index()\n",
    "header_items_lang =header_items_lang.drop(columns='index')\n",
    "\n",
    "# save translations of header flags\n",
    "header_items_lang.to_feather(language_flg_trans_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# load feather file for translation of header language flags\n",
    "header_items_lang = pd.read_feather(language_flg_trans_path)\n",
    "display(header_items_lang.head(2))\n",
    "\n",
    "if 'item_lang_en' in header_items_df.columns:\n",
    "    header_items_df = header_items_df.drop(columns='item_lang_en') \n",
    "\n",
    "# merge english translation of language flag to header items df\n",
    "header_items_df = header_items_df.merge(header_items_lang, left_on='item_lang_de', right_on='lang_de', how='left')\n",
    "\n",
    "# drop column with german translation\n",
    "header_items_df = header_items_df.drop(columns=['item_lang_de','language','lang_de'])\n",
    "header_items_df = header_items_df.rename(columns={'lang_en': 'item_lang_en'})\n",
    "\n",
    "# show df\n",
    "display(header_items_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate: language flag of items in evaluations_df\n",
    "\n",
    "Note: most book descriptions of books in evaluation_df are in top 5 languages as well:\n",
    "- de\ten\t es\t it\tfr\t |   or\tan\tbn\tfi\tno\tpt\n",
    "- 425\t380\t 22\t 7\t5\t |   2\t1\t1\t1\t1\t1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# get header itemID per item in evaluation_df\n",
    "evaluation_header_df = header_items_lookup_df.merge(evaluation_df, on='itemID',how='right')\n",
    "display(evaluation_header_df[evaluation_header_df['headerID'].isna()])\n",
    "\n",
    "# get language of book and description_lang \n",
    "evaluation_header_df = evaluation_header_df.merge(header_items_df[['headerID','lang_en','description_lang']], on='headerID', how='inner')\n",
    "\n",
    "# value counts: description_lang\n",
    "display(pd.DataFrame(evaluation_header_df[['description_lang']].value_counts()).transpose())\n",
    "\n",
    "# value counts: lang_en vs. description_lang\n",
    "display(pd.DataFrame(evaluation_header_df[['lang_en','description_lang']].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thalia Book Descriptions - Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "header_items_df.loc[:, 'description_'] = header_items_df.description.map(lambda x: next(iter(x)))\n",
    "\n",
    "# convert all strings to lowercase\n",
    "header_items_df['description_'] = header_items_df['description_'].apply(lambda s:s.lower() if type(s) == str else s)    \n",
    "header_items_df['description_'] = header_items_df['description_'].apply(remove_nontitle_substrings)\n",
    "header_items_df['description_'] = header_items_df['description_'].astype(str).apply(remove_special_characters)\n",
    "header_items_df['description_'] = header_items_df['description_'].apply(convert_umlaute)\n",
    "header_items_df['description_'] = header_items_df['description_'].apply(remove_next_sign)\n",
    "\n",
    "# reduce all spaces in the articles to single spaces\n",
    "header_items_df['description_'] = header_items_df['description_'].apply(remove_duplicate_whitespace)\n",
    "\n",
    "# display cleaned df head\n",
    "display(header_items_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# drop initial description\n",
    "header_items_df.drop('description', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df_backup = header_items_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# header_items_df = header_items_df_backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate df for missing items__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# read in last generated header_df\n",
    "header_items_df_prev = pd.read_feather('../data/processed/20210524_header_items_df.feather')\n",
    "\n",
    "# generate unique description / desc_lang / desc_en \n",
    "header_items_df_prev = header_items_df_prev[['description_','description_lang', 'description_en']].drop_duplicates()\n",
    "print('header_items_df_prev: ',header_items_df_prev.shape)\n",
    "header_items_df_prev.head(2)\n",
    "\n",
    "# merge both into new: header_items_df_ -> to get language flag and translation\n",
    "header_items_df_ = header_items_df.merge(header_items_df_prev, on='description_', how='left')\n",
    "print('header_items_df_: ', header_items_df_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# clean descriptions again\n",
    "cond_clean_desc = (header_items_df_['description_'] == ' ') | (header_items_df_['description_'] == '') | (header_items_df_['description_'] == 'na') | (header_items_df_['description_'] == 'nan')\n",
    "print('cond_clean_desc: ',cond_clean_desc.sum())\n",
    "header_items_df_['description_'] = np.where(cond_clean_desc, None, header_items_df_['description_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# inspect missing descriptions\n",
    "cond_missing_desc = header_items_df_['description_'].notna() & header_items_df_['description_en'].isna() & header_items_df_['description_lang'].isna()\n",
    "print('cond_missing_desc: ',cond_missing_desc.sum())\n",
    "header_items_df_missing = header_items_df_[cond_missing_desc] #289 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df_.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thalia Book Descriptions -  Translation (HEL NLP)\n",
    "\n",
    "__Approaches tried:__\n",
    "- tried Google Translator but not suitable as quite unreliable\n",
    "- will block after 20k requests\n",
    "\n",
    "__Current Approach:__\n",
    "- Helsinki NLP Translation Model (ran on Google Colab GPU)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## Google Translator\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "header_items_df[(header_items_df['language'] == {'Italienisch'}) & (header_items_df['description_']!='')]['description_'].apply(translator.translate, src='it',  dest='en' ).apply(getattr, args=('text',))\n",
    "\n",
    "# header_items_df['descriptions_trans'] = header_items_df['description_'].apply(translator.translate, dest='en' ).apply(getattr, args=('text',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification of language used for book descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(A) Old Version for entire df__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# if recompute_desc_lg_flg:\n",
    "\n",
    "#     # get start time for performance evaluation\n",
    "#     start_time_lg = time.time()\n",
    "\n",
    "#     # classify language of descriptions (note: language flag and description language don't necessarily fit !)\n",
    "#     # - only choose language of method classify (> ignore confidence score)\n",
    "#     header_items_df.loc[:,'description_lang'] = header_items_df['description_'].apply(lambda x: langid.classify(str(x))[0])\n",
    "\n",
    "#     # replace lang flg by none if no description given\n",
    "#     cond_no_desc = header_items_df['description_'].isna()\n",
    "#     header_items_df['description_lang'] = np.where(cond_no_desc, None, header_items_df['description_lang'])\n",
    "#     display(header_items_df.head())\n",
    "\n",
    "#     # compute execution time\n",
    "#     end_time_lg = time.time()\n",
    "#     print(f'exection time langid: {end_time_lg - start_time_lg} seconds') # takes ~ 5 mins to compute\n",
    "\n",
    "#     # save language flag for description to feather (for simplified reusability)\n",
    "#     header_items_df[['headerID','description_','description_lang']].to_feather(description_lang_path)\n",
    "#     print(f'description language exported to: {description_lang_path}')\n",
    "    \n",
    "# else:\n",
    "#     # load stored language flag\n",
    "#     if 'description_lang' not in header_items_df.columns:\n",
    "#         description_lang_df = pd.read_feather(description_lang_path).drop(columns='description_')\n",
    "#         header_items_df = header_items_df.merge(description_lang_df, on='headerID',how='left')\n",
    "#     display(header_items_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(B) Alternative Version for missing items__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # get start time for performance evaluation\n",
    "# start_time_lg = time.time()\n",
    "\n",
    "# # classify language of descriptions (note: language flag and description language don't necessarily fit !)\n",
    "# # - only choose language of method classify (> ignore confidence score)\n",
    "# header_items_df_missing.drop(columns='description_lang', inplace=True)\n",
    "# header_items_df_missing.loc[:,'description_lang'] = header_items_df_missing['description_'].apply(lambda x: langid.classify(str(x))[0])\n",
    "\n",
    "# # replace lang flg by none if no description given\n",
    "# cond_no_desc = header_items_df_missing['description_'].isna()\n",
    "# header_items_df_missing['description_lang'] = np.where(cond_no_desc, None, header_items_df_missing['description_lang'])\n",
    "# display(header_items_df_missing.head(2))\n",
    "\n",
    "# # compute execution time\n",
    "# end_time_lg = time.time()\n",
    "# print(f'exection time langid: {end_time_lg - start_time_lg} seconds') # takes ~ 5 mins to compute\n",
    "\n",
    "# # inspect distribution of languages\n",
    "# print(header_items_df_missing['description_lang'].value_counts())\n",
    "\n",
    "# # copy english descriptions\n",
    "# cond_en_desc = header_items_df_missing['description_lang'] == 'en'\n",
    "# header_items_df_missing['description_en'] = np.where(cond_en_desc, \n",
    "#                                                      header_items_df_missing['description_'],\n",
    "#                                                      header_items_df_missing['description_en'])\n",
    "# display(header_items_df_missing.head(2))\n",
    "\n",
    "# # select descriptions > only required when re-computing translations\n",
    "# header_items_df_missing_desc = header_items_df_missing[['description_','description_en','headerID','description_lang']]\n",
    "# header_items_df_missing_desc.reset_index(inplace=True)\n",
    "# header_items_df_missing_desc.drop(columns='index',inplace=True)\n",
    "# header_items_df_missing_desc.head(2)\n",
    "\n",
    "# # save as feather\n",
    "# header_items_df_missing_desc.to_feather(description_lang_missing_items_path)\n",
    "\n",
    "# read feather\n",
    "header_items_df_missing_desc = pd.read_feather(description_lang_missing_items_path)\n",
    "\n",
    "# merge with computed language descriptions\n",
    "header_items_df_missing.drop(columns=['description_lang','description_en'], inplace=True)\n",
    "header_items_df_missing = header_items_df_missing.merge(header_items_df_missing_desc[['headerID','description_lang']], \n",
    "                                                        on='headerID', how='left')\n",
    "print(header_items_df_missing.shape)\n",
    "display(header_items_df_missing.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Validation__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# inspect distribution of language flags\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# determine threshold for min. #books with certain language flag\n",
    "th = 100 \n",
    "\n",
    "# value counts for languagues with #books >= th\n",
    "lan_dist = pd.DataFrame(header_items_df['description_lang'].value_counts(dropna=False).reset_index())\n",
    "lan_dist_sub = lan_dist[lan_dist['description_lang']>=th]\n",
    "print(f'#distinct language combos in \"description_lang\" (th: >= {th}): {len(lan_dist_sub)}')\n",
    "display(lan_dist_sub)\n",
    "sns.barplot(y='index',x='description_lang', data=lan_dist_sub, orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# inspect missing lanuage flag -> langid always assigns language flag\n",
    "print(f'#items w/o description lang: {header_items_df[\"description_lang\"].isna().sum()}')\n",
    "print(f'#items w/o description: {header_items_df[\"description_\"].isna().sum()}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check fit between book language and description language\n",
    "show_no = 100\n",
    "pd.set_option(\"display.max_rows\", show_no)\n",
    "pd.DataFrame(header_items_df[['language','description_lang']].value_counts().reset_index()).head(show_no)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# inspect combinations of book language and description language\n",
    "language = \"{'Englisch'}\"\n",
    "desc = \"de\"\n",
    "\n",
    "cond_outlier_1 = (header_items_df['language'] == language) & (header_items_df['description_lang'] == desc)\n",
    "print(f'#items: {cond_outlier_1.sum()}')\n",
    "display(header_items_df[cond_outlier_1].head(5))\n",
    "# display(pd.DataFrame(header_items_df[cond_outlier_1]['description_'].value_counts().reset_index()).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute translations per language (only select top 5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if recompute_desc_trans:\n",
    "    pd.set_option(\"display.max_rows\", 10)\n",
    "\n",
    "    # determine th: min. #descriptions in certain language (to be considered for modelling/translation)\n",
    "    th = 100 \n",
    "\n",
    "    # get list of languages with #books >= th and description not null\n",
    "    lan_dist = pd.DataFrame(header_items_df['description_lang'].value_counts(dropna=True).reset_index())\n",
    "    frequent_desc_lang = list(lan_dist[(lan_dist['description_lang']>=th) & (lan_dist['index'].notna())]['index'])\n",
    "    frequent_desc_lang.remove('en')\n",
    "    print(f'list of languages with min. {th} book descriptions (w/o english): {frequent_desc_lang}')\n",
    "\n",
    "    # inspect duplication of descriptions\n",
    "    print(f'#descriptions occuring more than once: {(header_items_df[\"description_\"].value_counts() > 1).sum()}')\n",
    "\n",
    "    # generate descriptions df with unique descriptions to reduce workload\n",
    "    cond_desc = (header_items_df.description_.notna()) & (header_items_df.description_lang.isin(frequent_desc_lang))\n",
    "    hidf_sub = header_items_df.loc[cond_desc, [\"headerID\",\"description_\",\"description_lang\"]]\n",
    "\n",
    "    descriptions_df_description_lang = hidf_sub['headerID'].groupby([hidf_sub.description_]).apply(set).reset_index()\n",
    "    descriptions_df_headerID = hidf_sub['description_lang'].groupby([hidf_sub.description_]).apply(set).reset_index()\n",
    "\n",
    "    # merge all attributes\n",
    "    descriptions_df = reduce(lambda left,right: pd.merge(left,right,on=['description_'],\n",
    "                                                how='outer'), [descriptions_df_description_lang,\n",
    "                                                              descriptions_df_headerID])\n",
    "\n",
    "    # inspect descriptions_df\n",
    "    print(f'descriptions_df shape: {descriptions_df.shape}')\n",
    "    print(f'#elements with more than one language tag: {(descriptions_df[\"description_lang\"].apply(lambda x: len(x)) > 1).sum()}')\n",
    "\n",
    "    # remove set from description_lang if there is only one language tag per description\n",
    "    if (descriptions_df[\"description_lang\"].apply(lambda x: len(x)) > 1).sum() == 0:\n",
    "        descriptions_df[\"description_lang\"] = descriptions_df[\"description_lang\"].apply(lambda x: list(x)[0])\n",
    "    display(descriptions_df.tail(10))\n",
    "\n",
    "    # convert col values for export as feather\n",
    "    descriptions_df['headerID'] = descriptions_df['headerID'].astype(str)\n",
    "\n",
    "    # save subdf for specific language\n",
    "    # descriptions_df.to_feather(f'{description_trans_path}descriptions_df.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# reset list for testing\n",
    "\n",
    "# frequent_desc_lang = ['de', 'es', 'fr', 'it', 'pt']\n",
    "# frequent_desc_lang = ['es']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if recompute_desc_trans:\n",
    "\n",
    "    # initialize column containing all englisch descriptions\n",
    "    descriptions_df['description_en'] = None\n",
    "\n",
    "    # generate translations\n",
    "    for lang in frequent_desc_lang:\n",
    "\n",
    "      print(f'************************* {lang} *************************')\n",
    "\n",
    "      # get start time for performance evaluation\n",
    "      start_time_ld = time.time()\n",
    "      print(f'started at: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "\n",
    "      # set name to be used for model download (different e.g. for portuguese)\n",
    "      if lang == 'pt':\n",
    "          lang_flg = 'ROMANCE'\n",
    "      else:\n",
    "          lang_flg = lang\n",
    "\n",
    "      print(f'lang_flg: {lang_flg}')\n",
    "\n",
    "      # cnt descriptions in source language\n",
    "      total_cnt_desc = (descriptions_df[\"description_lang\"]== f'{lang}').sum()\n",
    "      print('#desc in {lang} to translate: {cnt}\\n'.format(lang=lang,\n",
    "                                                            cnt=total_cnt_desc))\n",
    "\n",
    "      # initialize list for storing translations\n",
    "      translations_lst = []\n",
    "      error_idx_lst = []\n",
    "\n",
    "      # select relevant item descriptions\n",
    "      cond_lang = (descriptions_df['description_lang'] == lang) & (cond_batch5)\n",
    "      descriptions_df_sub = descriptions_df.loc[cond_lang, 'description_'].reset_index()\n",
    "\n",
    "      for idx in descriptions_df_sub.index:\n",
    "\n",
    "          text = descriptions_df_sub.iloc[idx]['description_']\n",
    "\n",
    "          try: \n",
    "            # A larger beam size produces higher quality translations, \n",
    "            # but requires longer for the translation. \n",
    "            # By default, beam-size is set to 5.   \n",
    "            translations = model.translate(text, \n",
    "                                           source_lang=f'{lang_flg}', \n",
    "                                           target_lang='en',\n",
    "                                           beam_size=1)\n",
    "\n",
    "            # add translations to translations_lst\n",
    "            translations_lst.append(translations)\n",
    "\n",
    "          except:\n",
    "\n",
    "              print(f'*** error at (original) index: {descriptions_df_sub.iloc[idx][\"index\"]} ***')\n",
    "              error_idx_lst.append(descriptions_df_sub.iloc[idx][\"index\"])\n",
    "              translations_lst.append(None)\n",
    "\n",
    "          # print current status report\n",
    "          if (int(idx%100) == 0) & (idx!=0):\n",
    "            print(f'done: {idx}/{total_cnt_desc} - {datetime.now().strftime(\"%H:%M:%S\")}')   \n",
    "\n",
    "    #       # save current status\n",
    "    #       if (int(idx%200) == 0) & (idx!=0):\n",
    "    #           # generate sub df at current status\n",
    "    #           max_index = len(translations_lst)-1\n",
    "    #           desc_df_sub_copy = descriptions_df_sub.copy()\n",
    "    #           desc_df_sub_copy.loc[0:max_index, 'description_en'] = translations_lst\n",
    "    # #             display(desc_df_sub_copy)\n",
    "\n",
    "    #           # save subdf for specific language\n",
    "    #           desc_df_sub_copy.to_feather(f'sample_data/transl_{lang}_{idx}.feather')\n",
    "    #           files.download(f\"sample_data/transl_{lang}_{idx}.feather\") \n",
    "\n",
    "      # generate sub df at current status\n",
    "      max_index = len(translations_lst)-1\n",
    "      descriptions_df_sub.loc[0:max_index, 'description_en'] = translations_lst\n",
    "\n",
    "      # print list of idices with errors\n",
    "      print(f'list of errors: {error_idx_lst} *************************************\\n')\n",
    "\n",
    "      # save subdf for specific language\n",
    "      descriptions_df_sub.to_feather(f'sample_data/transl_{lang}_batch{batch_no}.feather')\n",
    "\n",
    "      # compute execution time\n",
    "      end_time_ld = time.time()\n",
    "      exec_time_secs = end_time_ld - start_time_ld\n",
    "      print(f'exection time translation {lang}: {exec_time_secs} secs or {exec_time_secs/60} mins')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# TESTING: can model for specific language combo be found?\n",
    "\n",
    "# specify the model name\n",
    "lang = \"ROMANCE\" # use for portuguese\n",
    "target_lang = \"en\"\n",
    "model_name = f'Helsinki-NLP/opus-mt-{lang}-{target_lang}'\n",
    "print(model_name)\n",
    "\n",
    "# Download the model and the tokenizer\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Main computation done in Google Colab (GPU) - Read in translated batches & merge to original df__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(A) Old Version: entire df__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if not recompute_desc_trans:\n",
    "    \n",
    "#     # read in existing translations\n",
    "#     # - batches for german translations\n",
    "#     desc_trans_de_b1 = pd.read_feather(description_trans_path + \"de/transl_de_batch1.feather\")\n",
    "#     desc_trans_de_b2 = pd.read_feather(description_trans_path + \"de/transl_de_batch2.feather\")\n",
    "#     desc_trans_de_b3 = pd.read_feather(description_trans_path + \"de/transl_de_batch3.feather\")\n",
    "#     desc_trans_de_b4 = pd.read_feather(description_trans_path + \"de/transl_de_batch4.feather\")\n",
    "#     desc_trans_de_b5 = pd.read_feather(description_trans_path + \"de/transl_de_batch5.feather\")\n",
    "    \n",
    "#     # - other language translations\n",
    "#     desc_trans_es = pd.read_feather(description_trans_path + \"es/transl_es_total.feather\")\n",
    "#     desc_trans_fr = pd.read_feather(description_trans_path + \"fr/transl_fr_total.feather\")\n",
    "#     desc_trans_it = pd.read_feather(description_trans_path + \"it/transl_it_total.feather\")\n",
    "#     desc_trans_pt = pd.read_feather(description_trans_path + \"pt/transl_pt_total.feather\")\n",
    "    \n",
    "# #     # display df heads\n",
    "# #     display(desc_trans_es.head(2))\n",
    "# #     display(desc_trans_fr.head(2))\n",
    "# #     display(desc_trans_it.head(2))\n",
    "# #     display(desc_trans_pt.head(2))\n",
    "    \n",
    "#     # merge all dfs\n",
    "#     translations = pd.concat([desc_trans_de_b1,desc_trans_de_b2,desc_trans_de_b3,desc_trans_de_b4,desc_trans_de_b5,\n",
    "#                               desc_trans_es, desc_trans_fr, desc_trans_it, desc_trans_pt])\n",
    "#     print(translations.shape)\n",
    "    \n",
    "#     # load stored descriptions_df\n",
    "#     descriptions_df = pd.read_feather(f'{description_trans_path}descriptions_df.feather').reset_index()\n",
    "#     print(descriptions_df.shape)\n",
    "    \n",
    "#     # merge translated descriptions with original descriptions df to get header ids\n",
    "#     translated_descriptions_df = descriptions_df.merge(translations[['index','description_en']], on='index', how='outer')\n",
    "\n",
    "#     # convert headerID from set to list\n",
    "#     translated_descriptions_df['headerID'] = translated_descriptions_df['headerID'].apply(lambda x: list(eval(x)))\n",
    "    \n",
    "#         # reset index\n",
    "#     translated_descriptions_df.drop(columns='index', inplace=True)\n",
    "#     translated_descriptions_df.reset_index(inplace=True)\n",
    "#     translated_descriptions_df.drop(columns='index', inplace=True)\n",
    "    \n",
    "#     # insped df\n",
    "#     display(translated_descriptions_df.head(2))\n",
    "    \n",
    "#     # save as feather file\n",
    "#     translated_descriptions_df.to_feather(description_trans_path + 'descriptions_translated_df.feather')\n",
    "    \n",
    "    # read stored feather file\n",
    "    translated_descriptions_df = pd.read_feather(description_trans_path + 'descriptions_translated_df.feather')\n",
    "\n",
    "    # explode dataframe: if row applies to multiple headerIDs, create one row per headerID\n",
    "    translated_descriptions_df = translated_descriptions_df.explode(column='headerID')\n",
    "\n",
    "    # missing values are only german\n",
    "    print(translated_descriptions_df[translated_descriptions_df['description_en'].isna()]['description_lang'].value_counts(dropna=False))\n",
    "    \n",
    "    # join translated descriptions to header_items_df\n",
    "    if 'description_en' not in header_items_df.columns:\n",
    "        header_items_df = header_items_df.merge(translated_descriptions_df[['headerID','description_en']], on='headerID', how='left')\n",
    "\n",
    "    # copy translation from description_ to description_en if description_lang == 'en'\n",
    "    cond_en = header_items_df['description_lang'] == 'en'\n",
    "    header_items_df['description_en'] = np.where(cond_en, header_items_df['description_'], header_items_df['description_en'])\n",
    "\n",
    "    display(header_items_df.tail(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# read in translations for missing items\n",
    "header_items_df_missing_transl_de = pd.read_feather(description_trans_path + 'de/transl_de_missing_header_items.feather')\n",
    "header_items_df_missing_transl_fr = pd.read_feather(description_trans_path + 'fr/transl_fr_missing_header_items.feather')\n",
    "header_items_df_missing_transl_it = pd.read_feather(description_trans_path + 'it/transl_it_missing_header_items.feather')\n",
    "header_items_df_missing_transl_sp = pd.read_feather(description_trans_path + 'es/transl_es_missing_header_items.feather')\n",
    "\n",
    "# combine them\n",
    "header_items_df_missing_transl = pd.concat([header_items_df_missing_transl_de,\n",
    "                                           header_items_df_missing_transl_fr,\n",
    "                                           header_items_df_missing_transl_it,\n",
    "                                           header_items_df_missing_transl_sp \n",
    "                                           ])\n",
    "\n",
    "# make sure unique values are copied\n",
    "header_items_df_missing_transl = header_items_df_missing_transl[['description_','description_en']].drop_duplicates()\n",
    "display(header_items_df_missing_transl.head())\n",
    "print(header_items_df_missing_transl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# set index for merge with original df\n",
    "header_items_df_missing_ = header_items_df_missing.merge(header_items_df_missing_transl, \n",
    "                                                    left_on='description_', \n",
    "                                                    right_on='description_', \n",
    "                                                    how='left')\n",
    "print('# duplicated headerIDs after merge: ', (header_items_df_missing_.headerID.value_counts() > 1).sum())\n",
    "header_items_df_missing_.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # re-set col for original desc\n",
    "# header_items_df_missing_['description_'] = header_items_df_missing_['description__x']\n",
    "# header_items_df_missing_.drop(columns=['description__x','description__y'],inplace=True)\n",
    "\n",
    "# choose en translations\n",
    "header_items_df_missing_['description_en'] = np.where(header_items_df_missing_['description_lang'] == 'en', \n",
    "                                                     header_items_df_missing_['description_'],\n",
    "                                                     header_items_df_missing_['description_en'],)\n",
    "\n",
    "# get list of missing headerIDS\n",
    "missing_headerIDlist = list(header_items_df_missing_['headerID'])\n",
    "\n",
    "# remove headerIDs of missing items from original dataframe and concat both dfs\n",
    "print('header_items_df_: ',header_items_df_.shape)\n",
    "header_items_df_ = header_items_df_[~header_items_df_['headerID'].isin(missing_headerIDlist)]\n",
    "\n",
    "# concat header_items_df_ and header_items_df_missing_ (> including new translations)\n",
    "header_items_df_ = pd.concat([header_items_df_, header_items_df_missing_])\n",
    "print('header_items_df_: ', header_items_df_.shape)\n",
    "display(header_items_df_.head(2))\n",
    "\n",
    "# re-set header_items_df_ to header_items_df\n",
    "header_items_df = header_items_df_\n",
    "print('# duplicated headerIDs in final header_items_df: ', (header_items_df.headerID.value_counts() > 1).sum())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# QA: \n",
    "\n",
    "# check whether still missing descriptions and in which language \n",
    "# -> finding: only due to missing german translations, no description overall or minority langs\n",
    "cond_missing_en = header_items_df['description_en'].isna()\n",
    "cond_missing_src = header_items_df['description_'].isna()\n",
    "print(header_items_df[cond_missing_en | cond_missing_src]['description_lang'].value_counts(dropna=False))\n",
    "\n",
    "# en     29883\n",
    "# NaN    13500\n",
    "# or        85\n",
    "# sv        72\n",
    "# nl        70\n",
    "# pl        33\n",
    "# no        32\n",
    "# sl        26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thalia Recommended age\n",
    "\n",
    "1. bin into children/ teenagers / adults\n",
    "2. fill empty values with adults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def f(cell):\n",
    "    cell = [int(x) for x in cell]\n",
    "    try:\n",
    "        if min(cell) < 3:\n",
    "            return 'newborn'\n",
    "        elif (min(cell) >=12) & (min(cell) <=18):\n",
    "            return 'teen'\n",
    "        elif (min(cell) > 7) & (min(cell) <12):\n",
    "            return 'child'\n",
    "        elif (min(cell) >=3) & (min(cell) <=7):\n",
    "            return 'small child'\n",
    "        else:\n",
    "            return 'adult'\n",
    "    except:\n",
    "        return 'adult'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# get rid of set structure (only get one value)\n",
    "header_items_df.loc[:, 'recommended_age'] = header_items_df.recommended_age.map(lambda x: next(iter(x)))\n",
    "header_items_df['rec_age'] = header_items_df.recommended_age.str.findall('\\d+')\n",
    "\n",
    "# TESTING\n",
    "# header_items_df['rec_age'].apply(lambda x: map(int, x))\n",
    "# header_items_df.recommended_age.str.replace('ab', )\n",
    "# header_items_df[header_items_df['recommended_age'].str.contains('5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# re-set NaN\n",
    "header_items_df['rec_age'] = np.where(header_items_df['rec_age'].isna(), ['99'], header_items_df['rec_age'])\n",
    "\n",
    "# generate new age col\n",
    "header_items_df['rec_age'] = header_items_df['rec_age'].apply(f)\n",
    "\n",
    "# inspect results\n",
    "print(header_items_df.rec_age.value_counts())\n",
    "display(header_items_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thalia Page Numbers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pre-process set of page numbers\n",
    "header_items_df['number_pages_2'] = header_items_df['number_pages'].apply(lambda x: re.sub(r\", ''\",'',x))\n",
    "header_items_df['number_pages_2'] = header_items_df['number_pages_2'].apply(unify_set_from_str)\n",
    "header_items_df['number_pages_2'] = header_items_df['number_pages_2'].apply(lambda x: x.split(','))\n",
    "print('header_items_df 1: ', header_items_df.shape)\n",
    "\n",
    "# comput #page elements\n",
    "header_items_df['np'] = header_items_df['number_pages_2'].apply(lambda x: len(x))\n",
    "display(header_items_df['np'].value_counts(dropna=False))\n",
    "print('header_items_df 2: ', header_items_df.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## np == 1\n",
    "\n",
    "cond_np_equal_1 = header_items_df['np']==1\n",
    "\n",
    "# compute difference\n",
    "np_diff = header_items_df[cond_np_equal_1][['number_pages_2']]\n",
    "np_diff['number_pages_2'] = np_diff['number_pages_2'].apply(lambda x: x[0])\n",
    "\n",
    "# reset empty list to 0\n",
    "cond_empty_list = np_diff['number_pages_2'] == ''\n",
    "print(f'#items with empty list of page nos: {cond_empty_list.sum()}')\n",
    "np_diff['number_pages_2'] = np.where(cond_empty_list, 0, np_diff['number_pages_2'])\n",
    "\n",
    "# reset nan to 0\n",
    "cond_nan = np_diff['number_pages_2'] == 'nan'\n",
    "print(f'#items with \"nan\": {cond_nan.sum()}')\n",
    "np_diff['number_pages_2'] = np.where(cond_nan, 0, np_diff['number_pages_2'])\n",
    "\n",
    "# convert to int\n",
    "np_diff['number_pages_2'] = np_diff['number_pages_2'].apply(lambda x: int(x))\n",
    "display(np_diff.head(2))\n",
    "\n",
    "# rename column before merging\n",
    "np_diff = np_diff.rename(columns={'number_pages_2': 'number_pages_1'})\n",
    "\n",
    "# merge both dfs \n",
    "if 'number_pages_1' in header_items_df.columns:\n",
    "    header_items_df = header_items_df.drop(columns='number_pages_1')\n",
    "print('header_items_df 3: ', header_items_df.shape)\n",
    "\n",
    "header_items_df = header_items_df.merge(np_diff, left_index=True, right_index=True, how='left')\n",
    "print('header_items_df 4: ', header_items_df.shape)\n",
    "\n",
    "# replace values\n",
    "header_items_df['number_pages_2'] = np.where(cond_np_equal_1, \n",
    "                                             header_items_df['number_pages_1'],\n",
    "                                             header_items_df['number_pages_2'])\n",
    "\n",
    "header_items_df['number_pages_2'] = np.where(header_items_df['number_pages_2']==0,\n",
    "                                             None,\n",
    "                                             header_items_df['number_pages_2'])\n",
    "header_items_df = header_items_df.drop(columns='number_pages_1')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## np == 2\n",
    "\n",
    "cond_np_equal_2 = header_items_df['np']==2\n",
    "\n",
    "# compute difference\n",
    "np_diff = header_items_df[cond_np_equal_2][['number_pages_2']]\n",
    "np_diff['np1'] = np_diff['number_pages_2'].apply(lambda x: int(x[0]))\n",
    "np_diff['np2'] = np_diff['number_pages_2'].apply(lambda x: int(x[1]))\n",
    "np_diff['diff'] = np_diff['np1'] - np_diff['np2']\n",
    "np_diff['avg'] = (np_diff['np1'] + np_diff['np2']) / 2\n",
    "display(np_diff.head(2))\n",
    "\n",
    "# merge both dfs \n",
    "header_items_df = header_items_df.merge(np_diff[['avg','diff']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# reset # pages if diff between both numbers > 100\n",
    "cond_np_above_th = header_items_df['diff'] > 100\n",
    "print(f'#items w. diff > 100: {cond_np_above_th.sum()}')\n",
    "header_items_df['number_pages_2'] = np.where(cond_np_above_th, \n",
    "                                             None, \n",
    "                                             header_items_df['number_pages_2'])\n",
    "\n",
    "# reset number of pages to avg\n",
    "cond_np_below_th = header_items_df['diff'] <= 100\n",
    "print(f'#items w. diff < 100: {cond_np_below_th.sum()}')\n",
    "header_items_df['number_pages_2'] = np.where(cond_np_below_th, \n",
    "                                             header_items_df['avg'], \n",
    "                                             header_items_df['number_pages_2'])\n",
    "header_items_df = header_items_df.drop(columns=['avg','diff'])\n",
    "\n",
    "## np > 2\n",
    "\n",
    "cond_np_morethan2 = header_items_df['np']>2\n",
    "\n",
    "# reset number of pages to None\n",
    "print(f'#items w. np > 2: {cond_np_below_th.sum()}')\n",
    "header_items_df['number_pages_2'] = np.where(cond_np_morethan2, \n",
    "                                             None, \n",
    "                                             header_items_df['number_pages_2'])\n",
    "\n",
    "# reset: number_pages_2 > number_pages\n",
    "header_items_df = header_items_df.drop(columns=['np','number_pages'])\n",
    "header_items_df = header_items_df.rename(columns={'number_pages_2': 'number_pages'})\n",
    "\n",
    "# inspect final df\n",
    "display(header_items_df.head(2))\n",
    "print(f'datatypes in col number_pages: \\n{header_items_df[\"number_pages\"].apply(type).value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# convert author and publischer to string instead of set\n",
    "header_items_df['author'] = header_items_df['author'].apply(lambda x: ' ,'.join(list((x))))\n",
    "header_items_df['publisher'] = header_items_df['publisher'].apply(lambda x: ' ,'.join(list((x))))\n",
    "header_items_df['mt_st_cl'] = header_items_df['mt_st_cl'].apply(lambda x: ' ,'.join(list((x))))\n",
    "header_items_df['number_pages'] = header_items_df['number_pages'].apply(lambda x: ' ,'.join(list((x))))\n",
    "display(header_items_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final validation for missing descriptions\n",
    "frequent_lang = ['de', 'es', 'pt', 'it', 'fr']\n",
    "for lang in frequent_lang:\n",
    "    cond = header_items_df['description_lang'] == lang\n",
    "    print(len(header_items_df[cond & header_items_df['description_en'].isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# final data checks\n",
    "print(header_items_df.shape)\n",
    "print(header_items_lookup_df.shape)\n",
    "print(header_items_lookup_df['headerID'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare items_df for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "items_df.drop(columns=['mt_cl','st_cl','rating','thalia_ranking','cover_url'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "items_df_copy = items_df.copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# items_df.loc[:, 'description'] = items_df.description.map(lambda x: next(iter(x)))\n",
    "\n",
    "# convert all strings to lowercase\n",
    "items_df['description'] = items_df['description'].apply(lambda s:s.lower() if type(s) == str else s)    \n",
    "items_df['description'] = items_df['description'].apply(remove_nontitle_substrings)\n",
    "items_df['description'] = items_df['description'].astype(str).apply(remove_special_characters)\n",
    "items_df['description'] = items_df['description'].apply(convert_umlaute)\n",
    "items_df['description'] = items_df['description'].apply(remove_next_sign)\n",
    "\n",
    "# reduce all spaces in the articles to single spaces\n",
    "items_df['description'] = items_df['description'].apply(remove_duplicate_whitespace)\n",
    "\n",
    "# display cleaned df head\n",
    "display(items_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export of final pre-processed dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# replace original cols by pre-processed cols\n",
    "items_df = items_df.rename(columns={'mt_cl': 'mt', 'st_cl': 'st', 'mt_st_cl': 'mt_st'})\n",
    "items_df.head(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# export items_df\n",
    "items_df.to_csv(items_path_pp, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# export transactions_df\n",
    "# transactions_df.to_csv(transactions_path_pp)\n",
    "transactions_df.to_csv('transactions_wo.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# header_items_df.reset_index(inplace=True)\n",
    "# header_items_df.drop(columns='index',inplace=True)\n",
    "\n",
    "# export header_items_df\n",
    "header_items_df.to_csv(header_items_path_pp, index=False) # header_items_df.to_csv('header_items_df.csv')\n",
    "# header_items_df.to_feather(re.sub('.csv','.feather',header_items_path_pp))\n",
    "\n",
    "# export lookup df for header items\n",
    "# header_items_df.to_csv(header_items_lookup_path_pp, index=False) \n",
    "# header_items_df.to_feather(re.sub('.csv','.feather',header_items_lookup_path_pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # test whether written output looks fine\n",
    "# pd.read_csv(header_items_lookup_path_pp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "426.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
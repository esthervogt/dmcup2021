{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Author: Esther Vogt\n",
    "# Creation Date: 11.05.2021\n",
    "# Purpose: Pre-processing of raw data + external data / Generation of header-itemset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Dos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Backlog:\n",
    "# - match google data?\n",
    "\n",
    "# Todo:\n",
    "# - description embeddings @Estelle\n",
    "# - clean header translation tags @Esther\n",
    "# - make translations per language (only select top 5?) @Esther\n",
    "# - detect whether description even is in source language @Esther\n",
    "# - clean up script a bit @Esther\n",
    "\n",
    "# In Progress:\n",
    "# - translation of descriptions @Esther\n",
    "\n",
    "# Done:\n",
    "# - titel preprocessing: stop word removal / Kommentare wie taschenbuch/editions etc. @Esther\n",
    "# - create superset @Esther \n",
    "# - pull main topic and sub topic together @Esther\n",
    "# - sentence embeddings @Estelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings & User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Imports & Settings\n",
    "########################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pycountry\n",
    "from pandas.core.common import flatten\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# allow display of all rows (with scrollbar)\n",
    "pd.set_option(\"display.max_rows\", 10) #pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# User Input\n",
    "########################################################################################################################\n",
    "\n",
    "# source data file paths\n",
    "transactions_path = '../data/external/transactions.csv'\n",
    "evaluation_path = '../data/external/evaluation.csv'\n",
    "items_path = '../data/external/items.csv'\n",
    "subject_cats_0_path = '../data/external/subject_cats_0.csv'\n",
    "gbooks_path = '../data/external/gbooks_final.json'\n",
    "\n",
    "# pre-processed data file paths (incl. language flags)\n",
    "transactions_path_pp = '../data/processed/transactions_pp.csv'\n",
    "items_path_pp = '../data/processed/items_pp.csv'\n",
    "header_items_path_pp = '../data/processed/header_items_pp.csv'\n",
    "gbooks_volumeInfo_path_pp = '../data/processed/gbooks_volumeInfo_pp.feather'\n",
    "header_items_20210517_path = '../data/processed/20210517_header_items_df.csv'\n",
    "header_items_20210519_path = '../data/processed/20210519_header_items_df.csv'\n",
    "\n",
    "# seaborn color palette\n",
    "palette_blue = \"Blues_d\"\n",
    "dark_blue = \"#011f4b\"\n",
    "middle_blue = \"#005b96\"\n",
    "light_blue = \"#b3cde0\"\n",
    "\n",
    "# determine: re-calculate certain details\n",
    "recompute_lg_flg = False # calculated language flags \n",
    "recompute_gbooks_volumeInfo = False # volumeInfo per book pulled from GoogleAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Functions\n",
    "########################################################################################################################\n",
    "\n",
    "def clean_alt_list(list_):\n",
    "#     list_ = list_.replace(', ', ',')\n",
    "    list_ = list_.replace('[', '')\n",
    "    list_ = list_.replace(']', '')\n",
    "    return list_\n",
    "\n",
    "\n",
    "def items_initial_col_processing(items_df, drop_original=True):\n",
    "    # add col: get len of mt string\n",
    "#     items_df['mt_len'] = items_df['main topic'].str.len()\n",
    "\n",
    "    # add col: get first element (top level category) of mt string\n",
    "#     items_df['mt_0'] = items_df['main topic'].str[0]\n",
    "\n",
    "    # add col: main topic as set (and converted back to list)\n",
    "    items_df['mt_cl'] = items_df['main topic'].astype(str).apply(lambda x: list(set(clean_alt_list(x).split(','))))\n",
    "\n",
    "    # adjust subtopics: set to None if subtopics list is empty\n",
    "    items_df['st_cl'] = items_df['subtopics'].astype(str).apply(lambda x: list(set(clean_alt_list(x).split(','))))\n",
    "    items_df.loc[items_df['st_cl']=={''}, 'st_cl'] = None\n",
    "\n",
    "    # add col: unique combination of main and subtopic\n",
    "    items_df['mt_st_cl'] = (items_df['st_cl'] + items_df['mt_cl']) #.apply(set)\n",
    "    \n",
    "    # drop initial topic cols\n",
    "    if drop_original:\n",
    "        items_df = items_df.drop(columns=['main topic', 'subtopics'])\n",
    "    \n",
    "    return items_df\n",
    "\n",
    "\n",
    "def tr_initial_col_processing(transactions_df):\n",
    "    # add col: get click / basket / order flag\n",
    "    transactions_df['click_flg'] = np.where(transactions_df['click'] > 0, 1, 0)\n",
    "    transactions_df['basket_flg'] = np.where(transactions_df['basket'] > 0, 1, 0)\n",
    "    transactions_df['order_flg'] = np.where(transactions_df['order'] > 0, 1, 0) \n",
    "    \n",
    "    return transactions_df\n",
    "\n",
    "\n",
    "def extract_gbook_volumeInfo(data, target_keys):\n",
    "\n",
    "    # initialize final details df\n",
    "    volumeInfo_df = pd.DataFrame()\n",
    "    total = len(data)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "    \n",
    "        # print progress report\n",
    "        if int(index%1000) == 0:\n",
    "            print(f'{index}/{total}')\n",
    "    \n",
    "        # extract volumInfo if given\n",
    "        if row[\"items\"]:\n",
    "            for item in row[\"items\"]:\n",
    "\n",
    "                available_keys = list(item['volumeInfo'].keys())\n",
    "    #             print(f'available_keys: {available_keys}')\n",
    "\n",
    "                extraction_keys = list(frozenset(available_keys).intersection(target_keys))\n",
    "    #             print(f'extraction keys: {extraction_keys}')\n",
    "\n",
    "                volumeInfo_item_df = pd.DataFrame(item).loc[extraction_keys,'volumeInfo']\n",
    "                volumeInfo_item_df = pd.DataFrame(volumeInfo_item_df).transpose()\n",
    "                volumeInfo_item_df[\"itemIdx\"] = row[\"itemIdx\"]\n",
    "    #             display(volumeInfo_item_df)\n",
    "    #             print()\n",
    "\n",
    "                volumeInfo_df = pd.concat([volumeInfo_df,volumeInfo_item_df])\n",
    "\n",
    "    # reset index of volumeInfo df\n",
    "    volumeInfo_df.reset_index(inplace=True)\n",
    "    volumeInfo_df = volumeInfo_df.drop(columns='index')  \n",
    "    \n",
    "    return volumeInfo_df \n",
    "\n",
    "\n",
    "def remove_special_characters(list_):\n",
    "#     list_ = re.sub(r'^\\W+', r'', list_) #removes leading non-alphanumerics, e.g. \",william shakespeare\"\n",
    "\n",
    "    # Remove punctuation & special characters\n",
    "    list_ = re.sub(r'[®,\\.!?\\\"\\(\\)\\'\\:#]','',list_)\n",
    "    list_ = re.sub(r'-',' ',list_)\n",
    "    return list_\n",
    "\n",
    "\n",
    "def remove_next_sign(list_):\n",
    "#     list_ = re.sub(r'^\\W+', r'', list_) #removes leading non-alphanumerics, e.g. \",william shakespeare\"\n",
    "\n",
    "    # Remove punctuation & special characters\n",
    "    list_ = re.sub(r'[\\n]','',list_)\n",
    "    \n",
    "    return list_\n",
    "\n",
    "def remove_nontitle_substrings(list_):\n",
    "    list_ = str(list_)\n",
    "\n",
    "    # type of book\n",
    "    for book_type in ['taschenbuch','hardcover','hardback']:\n",
    "        list_ = re.sub(f'\\(.*{book_type}.*\\)?','',list_) #remove all content within brackets\n",
    "        list_ = re.sub(f'-\\s*(\\w*\\s*){book_type}.*','',list_)\n",
    "        list_ = re.sub(f':.*{book_type}.*','',list_)\n",
    "        list_ = re.sub(f'(.*{book_type}[\\w\\d\\s]*):','',list_)\n",
    "        list_ = re.sub(f'[(special)(book)(edition)\\s*]*{book_type}\\s*[(special)(book)(edition)\\s*]*','',list_)\n",
    "        list_ = re.sub(f'{book_type}','',list_)\n",
    "        \n",
    "    # (light novel)\n",
    "    list_ = re.sub(f'(light novel)','',list_)\n",
    "    list_ = re.sub(f'\\(novel\\)','',list_)\n",
    "    \n",
    "    # (edition)\n",
    "    list_ = re.sub(f'\\(.*edition.*\\)','',list_)  \n",
    "\n",
    "    return list_\n",
    "\n",
    "\n",
    "def convert_umlaute(list_):\n",
    "    list_ = list_.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\")\n",
    "    return list_\n",
    "\n",
    "\n",
    "def remove_duplicate_whitespace(list_):\n",
    "    list_ = re.sub(f' {2,}','',list_)\n",
    "    return list_\n",
    "\n",
    "\n",
    "def generate_header_set(items_df):\n",
    "    \"\"\"\n",
    "    generates header set of items that combines attributes of several items with same title that e.g. only differ in itemID\n",
    "    or other attributes\n",
    "    > headerID can be used to replace itemID in transactions_df\n",
    "    \"\"\"\n",
    "    # generate header attribute sets from sub-items -> important: generate sets to prevent duplication \n",
    "    header_items_author_df = items_df['author'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    header_items_publisher_df = items_df['publisher'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    header_items_mtst_df = items_df['mt_st_cl'].groupby([items_df.title]).apply(sum).apply(set).reset_index() # get unique list of topics\n",
    "\n",
    "    header_items_language_df = items_df['language'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    header_items_number_pages_df = items_df['number_pages'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    header_items_recommended_age_df = items_df['recommended_age'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    header_items_release_date_df = items_df['release_date'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    header_items_description_df = items_df['description'].groupby([items_df.title]).apply(set).reset_index()\n",
    "    \n",
    "    # compile the list of dataframes you want to merge\n",
    "    header_items_df_lst = [header_items_author_df, header_items_publisher_df, header_items_mtst_df, header_items_language_df,\n",
    "                           header_items_number_pages_df,header_items_recommended_age_df, header_items_release_date_df,\n",
    "                           header_items_description_df ]\n",
    "\n",
    "    # merge all attributes\n",
    "    header_items_df = reduce(lambda left,right: pd.merge(left,right,on=['title'],\n",
    "                                                how='outer'), header_items_df_lst)\n",
    "\n",
    "    # generate new header index\n",
    "    header_items_df = header_items_df.reset_index().rename(columns={'index':'headerID'})\n",
    "\n",
    "    # result inspection\n",
    "    print(f'shape of header_items_df vs. items_df: {header_items_df.shape} vs. {items_df.shape}')\n",
    "    print(f'cnt of duplicate \"title\" in header_df: {(header_items_df[\"title\"].value_counts() > 1).sum()}')\n",
    "\n",
    "#     print(f'\\nconverted df:')\n",
    "#     display(header_items_df[header_items_df['title'].isin(['(Heli-)opolis - Der verhängnisvolle Plan des Weltkoordinators',\n",
    "#                                                    '13 Kings',\n",
    "#                                                    'Ära der Lichtwächter'])].head(5))\n",
    "\n",
    "#     print(f'\\noriginal df:')\n",
    "#     display(items_df[items_df['title'].isin(['(Heli-)opolis - Der verhängnisvolle Plan des Weltkoordinators',\n",
    "#                                                    '13 Kings',\n",
    "#                                                    'Ära der Lichtwächter'])].head(5))\n",
    "\n",
    "    return header_items_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load & initial pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DMC Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Load Data\n",
    "########################################################################################################################\n",
    "\n",
    "# Load the dmc source data\n",
    "\n",
    "# - clicks/baskets/order over a period of 3M\n",
    "# - rows: one transaction for single item\n",
    "transactions_df = pd.read_csv(transactions_path, delimiter='|', sep='.', encoding='utf-8')\n",
    "\n",
    "# - list of product ids (subset of products from items_df) to be used for prediction\n",
    "evaluation_df = pd.read_csv(evaluation_path, sep='.', encoding='utf-8')\n",
    "items_df = pd.read_csv(items_path, delimiter='|', sep='.', encoding='utf-8')\n",
    "\n",
    "# load category lookup table (manually created)\n",
    "subject_cats_0 = pd.read_csv(subject_cats_0_path, delimiter=';', encoding='utf-8')\n",
    "\n",
    "########################################################################################################################\n",
    "# Preprocessing for further inspection\n",
    "########################################################################################################################\n",
    "\n",
    "# extract list of base cols\n",
    "initial_cols= list(items_df.columns)\n",
    "\n",
    "# add/pre-process cols\n",
    "items_df = items_initial_col_processing(items_df, drop_original=True)\n",
    "transactions_df = tr_initial_col_processing(transactions_df)\n",
    "\n",
    "########################################################################################################################\n",
    "# Inspection of dfs after initial pre-processing\n",
    "########################################################################################################################\n",
    "\n",
    "# show dfs after initial pre-processing\n",
    "print(f'items_df after first pre-processing:')\n",
    "display(items_df.head(2))\n",
    "\n",
    "print(f'transactions_df after first pre-processing:')\n",
    "display(transactions_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processed Header DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load header df \n",
    "header_items_20210517 = pd.read_csv(header_items_20210517_path)\n",
    "header_items_20210517.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DEV] Google API Extract\n",
    "\n",
    "__To do:__\n",
    "1. process remaining batches\n",
    "2. reduce to one match per item\n",
    "3. include details into items_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Load & Pre-Processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if recompute_gbooks_volumeInfo:\n",
    "    # TODO: insert calculation of volumeInfo\n",
    "else:\n",
    "    volumeInfo_df = pd.read_feather(gbooks_volumeInfo_path_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the gbooks details (df)\n",
    "gbooks_df = pd.read_json(gbooks_path, orient='records')\n",
    "\n",
    "# reset index (to simplify later join with items_df)\n",
    "if 'index' in gbooks_df.columns:\n",
    "    gbooks_df = gbooks_df.drop(columns='index')\n",
    "gbooks_df.reset_index(inplace=True)\n",
    "gbooks_df = gbooks_df.rename(columns={'index':'itemIdx'})\n",
    "\n",
    "# get df stats\n",
    "print(f'gbooks_df:')\n",
    "display(gbooks_df.head())\n",
    "\n",
    "print(f'shape gbooks_df: {gbooks_df.shape}')\n",
    "print(f'shape items_df: {items_df.shape}\\n')\n",
    "\n",
    "# inspect distribution ot total items\n",
    "# plt.hist(gbooks_df['totalItems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "len(gbooks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 11.05.2021 - 09:57 - max batch index = 20k\n",
    "# 11.05.2021 - 10:31 - 20k-30k\n",
    "# 11.05.2021 - 10:55 - 30k-40k\n",
    "\n",
    "batch_start_index = 40000\n",
    "batch_end_index = len(gbooks_df)\n",
    "volumeInfo_df = extract_gbook_volumeInfo(gbooks_df.iloc[batch_start_index+1:batch_end_index+1,:],\n",
    "                                        target_keys=['title','publisher','authors','publishedDate','description','printType',\n",
    "                                                       'categories','maturityRating', 'language'])\n",
    "# inspect head of df\n",
    "# display(volumeInfo_df.head())\n",
    "\n",
    "# shape\n",
    "print(f'shape volumeInfo_df: {volumeInfo_df.shape}\\n')\n",
    "\n",
    "# get cnt of nas\n",
    "print(f'na per col: \\n{volumeInfo_df.isna().sum()}\\n')\n",
    "\n",
    "# value counts specific cols\n",
    "for col in ['maturityRating', 'printType','language']:\n",
    "    display(pd.DataFrame(volumeInfo_df[col].value_counts()).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "volumeInfo_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# exclude magazines\n",
    "volumeInfo_df = volumeInfo_df.loc[volumeInfo_df['printType']=='BOOK',:]\n",
    "# volumeInfo_df = volumeInfo_df.drop(columns='printType') #check whether only books in subsequent batches\n",
    "\n",
    "# reset index before saving as feather\n",
    "volumeInfo_df.reset_index(inplace=True)\n",
    "volumeInfo_df = volumeInfo_df.drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# save table as feather file (for simplified later load)\n",
    "gbooks_volumeInfo_path_pp = f'../data/interim/gbooks_volumeInfo_{int(batch_start_index / 1000)}k-{int(batch_end_index / 1000)}k.feather'\n",
    "volumeInfo_df.to_feather(gbooks_volumeInfo_path_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_itemIdx = 9378\n",
    "\n",
    "display(volumeInfo_df.loc[volumeInfo_df['itemIdx']==test_itemIdx,:])\n",
    "display(items_df.iloc[test_itemIdx,:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check all items with just one match\n",
    "gbooks_df.loc[gbooks_df['totalItems'] ==1 ]\n",
    "\n",
    "# compare items_df vs. gbooks_df\n",
    "display(items_df.iloc[2,:])\n",
    "print()\n",
    "print(gbooks_df.iloc[2,0:2])\n",
    "display(gbooks_df.iloc[2,2])\n",
    "\n",
    "# generate subset of gbooks details for testing and further pre-processing\n",
    "gbooks_df_sub = gbooks_df.iloc[0:100,:]\n",
    "display(gbooks_df_sub)\n",
    "\n",
    "# generate json\n",
    "gbooks_json_sub = gbooks_df_sub.to_dict()\n",
    "# gbooks_json_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thalia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "thalia_data = pd.read_pickle('../data/external/thalia_features.pkl')\n",
    "print(thalia_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "items_df = items_df.merge(thalia_data, on='itemID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DEV] Outlier Detection\n",
    "- only for __transactions__: remove transactions with suspiciously high #of clicks/basket/order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Original shape:', transactions_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(transactions_df['click'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "transactions_df = transactions_df[transactions_df['click'] < np.quantile(transactions_df.click, 0.99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(transactions_df['basket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "transactions_df = transactions_df[transactions_df['basket'] < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(transactions_df['order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "transactions_df = transactions_df[transactions_df['order'] < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('After outlier removal shape:', transactions_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String normalization\n",
    "\n",
    "__Applied:__\n",
    "1. conversion to lowercase, e.g. publisher = 'TEKTIME' or 'Tektime' to 'tektime'\n",
    "2. removal of leading special characters, e.g. \",william shakespeare\"\n",
    "3. conversion of unicode characters (ä,ö,ü)\n",
    "\n",
    "__No fix yet:__\n",
    "1. weird entries\n",
    "    - author: der Authhhhor\n",
    "    - diverse Autoren, Autoren\n",
    "3. unicode characters like (à,é,è,°o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate copy of original df for testing of pre-processing\n",
    "items_df_cl = items_df.copy()\n",
    "display(items_df_cl.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "items_df_cl = items_df.copy()\n",
    "display(items_df_cl.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cols_pp = ['title', 'author', 'publisher']\n",
    "\n",
    "# convert all strings to lowercase\n",
    "items_df[cols_pp] = items_df[cols_pp].applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "for col in cols_pp:\n",
    "    \n",
    "    col_cl = col + '_cl'\n",
    "\n",
    "    # add additional col for pp titles\n",
    "    items_df[col_cl] = items_df[col]\n",
    "\n",
    "    # clean strings\n",
    "    if col == 'title':\n",
    "        items_df[col_cl] = items_df[col_cl].apply(remove_nontitle_substrings)\n",
    "    items_df[col_cl] = items_df[col_cl].astype(str).apply(remove_special_characters)\n",
    "    items_df[col_cl] = items_df[col_cl].apply(convert_umlaute)\n",
    "\n",
    "    # reduce all spaces in the articles to single spaces\n",
    "    items_df[col_cl] = items_df[col_cl].apply(remove_duplicate_whitespace)\n",
    "\n",
    "    # print stats\n",
    "    col_cnt_unique = items_df[col].nunique()\n",
    "    col_cl_cnt_unique = items_df[col_cl].nunique()\n",
    "    print(f'# unique {col} (before preprocessing): {col_cnt_unique} / {len(items_df)}')\n",
    "    print(f'# unique {col} (after preprocessing): {col_cnt_unique} / {len(items_df)}')\n",
    "    print(f'# reduction in unique {col}: {col_cnt_unique-col_cl_cnt_unique}\\n')\n",
    "    \n",
    "# replace original cols by pre-processed cols\n",
    "items_df = items_df.drop(columns=cols_pp)\n",
    "items_df = items_df.rename(columns={'title_cl': 'title', 'author_cl': 'author', 'publisher_cl': 'publisher'})\n",
    "\n",
    "# remove items with missing title after pre-processing\n",
    "print(f\"remove items with missing/empty title after pp: {(items_df['title']=='').sum()}\")\n",
    "items_df = items_df[items_df['title']!='']\n",
    "\n",
    "# display cleaned df head\n",
    "display(items_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate titles df (with comparison column for original and cleaned title)\n",
    "titles_df = pd.DataFrame(items_df_cl[\"title\"].unique()).rename(columns={0: \"title\"})\n",
    "titles_df['title_cl'] = titles_df['title']\n",
    "\n",
    "# convert all strings to lowercase\n",
    "titles_df = titles_df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "# clean strings\n",
    "titles_df['title_cl'] = titles_df['title_cl'].astype(str).apply(remove_special_characters)\n",
    "titles_df['title_cl'] = titles_df['title_cl'].apply(remove_nontitle_substrings)\n",
    "titles_df['title_cl'] = titles_df['title_cl'].apply(convert_umlaute)\n",
    "\n",
    "# reduce all spaces in the articles to single spaces\n",
    "titles_df['title_cl'] = titles_df['title_cl'].apply(remove_duplicate_whitespace)\n",
    "\n",
    "# print stats\n",
    "title_cnt_unique = titles_df[\"title\"].nunique()\n",
    "title_cl_cnt_unique = titles_df[\"title_cl\"].nunique()\n",
    "print(f'# unique titles (before preprocessing): {title_cnt_unique} / {len(titles_df)}')\n",
    "print(f'# unique titles (after preprocessing): {title_cl_cnt_unique} / {len(titles_df)}')\n",
    "print(f'# reduction in unique titles: {title_cnt_unique-title_cl_cnt_unique}')\n",
    "\n",
    "# display cleaned df head\n",
    "display(titles_df.head(10))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "test_itemIdx = 3592\n",
    "\n",
    "display(volumeInfo_df.loc[volumeInfo_df['itemIdx']==test_itemIdx,:])\n",
    "display(items_df.iloc[test_itemIdx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Testing of removal    \n",
    "for book in ['unsterblich 02 - tor der nacht','meine kindergarten-freunde (pferde)']:\n",
    "    book = re.sub(r'-',' ',book)\n",
    "    print(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print cnt of items including special terms\n",
    "print(f'#items with title including:')\n",
    "col = \"title_cl\"\n",
    "for entry in ['hardcover','taschenbuch','edition','novel','hardback']:\n",
    "    cnt = titles_df[col].str.contains(f'{entry}').sum()\n",
    "    print(f'\\t{entry}: {cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# search for specific entry\n",
    "#pd.set_option(\"display.max_rows\", None)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#search_entry = r' +'\n",
    "#display(titles_df.loc[titles_df['title'].str.contains(f'{search_entry}'), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inspect matches for specific terms/patterns\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "# p = re.compile('\\(.*\\)')\n",
    "p = re.compile(r'edition')\n",
    "col = \"title_cl\"\n",
    "matches = titles_df[col].apply(lambda s: p.findall(s))\n",
    "matches = pd.DataFrame(set(flatten([x for x in matches if x])))\n",
    "matches\n",
    "\n",
    "\n",
    "# (1) -> elfengeist (1)\n",
    "# (dt. ausgabe)\n",
    "# the dark artifices box set (3 bände im schuber)\n",
    "# star wars(tm) - schülerin der dunklen seite\n",
    "# (sammelband) / (filmausgabe)\n",
    "# (neuauflage) / (sonderausgabe)\n",
    "# (roman) / (light novel)\n",
    "# (großdruck)\n",
    "# (gift edition) / (signed limited edition)\n",
    "# (manga)\n",
    "# (1-3 jahre)\n",
    "# (greek edition) / (german edition) / (greek book for kids) -> additional column with language tag extracted?\n",
    "# (spanish language edition of the things m -> check if error during reading in\n",
    "# (hardback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# cnt unique items per title\n",
    "title_cnt_bpp = titles_df.groupby('title').count().reset_index().rename(columns={'title_cl': 'cnt'})\n",
    "title_cnt_app = titles_df.groupby('title_cl').count().reset_index().rename(columns={'title': 'cnt'})\n",
    "# display(title_cnt_bpp)\n",
    "# display(title_cnt_app)\n",
    "\n",
    "# merge both cnts to get comparison\n",
    "titles_w_cnt = titles_df.merge(title_cnt_bpp, on='title', how='left')\n",
    "titles_w_cnt = titles_w_cnt.merge(title_cnt_app, on='title_cl', how='left')\n",
    "# display(titles_w_cnt)\n",
    "\n",
    "# inspect differences\n",
    "print(f'items with additional title matches: {len(titles_w_cnt[(titles_w_cnt[\"cnt_x\"] < titles_w_cnt[\"cnt_y\"])].drop_duplicates())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "display(titles_w_cnt[(titles_w_cnt['cnt_x'] < titles_w_cnt['cnt_y']) & \n",
    "                     (titles_w_cnt['cnt_y'] > 1)].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# inspect exemplary item\n",
    "titles_df[titles_df['title_cl'] == 'the dungeon masters wife']\n",
    "titles_df[titles_df['title_cl'] == 'z rex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# generate authors df (with comparison column for original and cleaned author)\n",
    "author_df = pd.DataFrame(items_df_cl[\"author\"].unique()).rename(columns={0: \"author\"})\n",
    "author_df['author_cl'] = author_df['author']\n",
    "\n",
    "# convert all strings to lowercase\n",
    "author_df = author_df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "# clean strings\n",
    "author_df['author_cl'] = author_df['author_cl'].astype(str).apply(remove_special_characters)\n",
    "author_df['author_cl'] = author_df['author_cl'].apply(convert_umlaute)\n",
    "\n",
    "# reduce all spaces in the articles to single spaces\n",
    "author_df['author_cl'] = author_df['author_cl'].apply(remove_duplicate_whitespace)\n",
    "\n",
    "# print stats\n",
    "author_cnt_unique = author_df[\"author\"].nunique()\n",
    "author_cl_cnt_unique = author_df[\"author_cl\"].nunique()\n",
    "print(f'# unique authors (before preprocessing): {author_cnt_unique} / {len(author_df)}')\n",
    "print(f'# unique authors (after preprocessing): {author_cl_cnt_unique} / {len(author_df)}')\n",
    "print(f'# reduction in unique authors: {author_cnt_unique-author_cl_cnt_unique}')\n",
    "\n",
    "# display cleaned df head\n",
    "display(author_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# cnt unique items per author\n",
    "author_cnt_bpp = author_df.groupby('author').count().reset_index().rename(columns={'author_cl': 'cnt'})\n",
    "author_cnt_app = author_df.groupby('author_cl').count().reset_index().rename(columns={'author': 'cnt'})\n",
    "# display(author_cnt_bpp)\n",
    "# display(author_cnt_app)\n",
    "\n",
    "# merge both cnts to get comparison\n",
    "authors_w_cnt = author_df.merge(author_cnt_bpp, on='author', how='left')\n",
    "authors_w_cnt = authors_w_cnt.merge(author_cnt_app, on='author_cl', how='left')\n",
    "# display(authors_w_cnt)\n",
    "\n",
    "# inspect differences\n",
    "print(f'items with additional author matches: {len(authors_w_cnt[(authors_w_cnt[\"cnt_x\"] < authors_w_cnt[\"cnt_y\"])].drop_duplicates())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "display(authors_w_cnt[(authors_w_cnt['cnt_x'] < authors_w_cnt['cnt_y']) & \n",
    "                     (authors_w_cnt['cnt_y'] > 1)].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# inspect exemplary item\n",
    "author_df[author_df['author_cl'] == 'larry w miller jr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# generate publishers df (with comparison column for original and cleaned publisher)\n",
    "publisher_df = pd.DataFrame(items_df_cl[\"publisher\"].unique()).rename(columns={0: \"publisher\"})\n",
    "publisher_df['publisher_cl'] = publisher_df['publisher']\n",
    "\n",
    "# convert all strings to lowercase\n",
    "publisher_df = publisher_df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "# clean strings\n",
    "publisher_df['publisher_cl'] = publisher_df['publisher_cl'].astype(str).apply(remove_special_characters)\n",
    "publisher_df['publisher_cl'] = publisher_df['publisher_cl'].apply(convert_umlaute)\n",
    "\n",
    "# reduce all spaces in the articles to single spaces\n",
    "publisher_df['publisher_cl'] = publisher_df['publisher_cl'].apply(remove_duplicate_whitespace)\n",
    "\n",
    "# print stats\n",
    "publisher_cnt_unique = publisher_df[\"publisher\"].nunique()\n",
    "publisher_cl_cnt_unique = publisher_df[\"publisher_cl\"].nunique()\n",
    "print(f'# unique publishers (before preprocessing): {publisher_cnt_unique} / {len(publisher_df)}')\n",
    "print(f'# unique publishers (after preprocessing): {publisher_cl_cnt_unique} / {len(publisher_df)}')\n",
    "print(f'# reduction in unique publishers: {publisher_cnt_unique-publisher_cl_cnt_unique}')\n",
    "\n",
    "# display cleaned df head\n",
    "display(publisher_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# cnt unique items per publisher\n",
    "publisher_cnt_bpp = publisher_df.groupby('publisher').count().reset_index().rename(columns={'publisher_cl': 'cnt'})\n",
    "publisher_cnt_app = publisher_df.groupby('publisher_cl').count().reset_index().rename(columns={'publisher': 'cnt'})\n",
    "# display(publisher_cnt_bpp)\n",
    "# display(publisher_cnt_app)\n",
    "\n",
    "# merge both cnts to get comparison\n",
    "publishers_w_cnt = publisher_df.merge(publisher_cnt_bpp, on='publisher', how='left')\n",
    "publishers_w_cnt = publishers_w_cnt.merge(publisher_cnt_app, on='publisher_cl', how='left')\n",
    "# display(publishers_w_cnt)\n",
    "\n",
    "# inspect differences\n",
    "print(f'items with additional publisher matches: {len(publishers_w_cnt[(publishers_w_cnt[\"cnt_x\"] < publishers_w_cnt[\"cnt_y\"])].drop_duplicates())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "display(publishers_w_cnt[(publishers_w_cnt['cnt_x'] < publishers_w_cnt['cnt_y']) & \n",
    "                     (publishers_w_cnt['cnt_y'] > 1)].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# inspect exemplary item\n",
    "publisher_df[publisher_df['publisher_cl'] == 'digital scanning inc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header-Set \n",
    "\n",
    "__Approach:__\n",
    "1. __[done]__ Generate new header-set with new IDs to unify same books that appear multiple times in the items and transactions table\n",
    "    a. generate new IDs\n",
    "    b. unify information\n",
    "2. __[done]__ Replace the subset IDs in transactions table by superset IDs\n",
    "\n",
    "3. Pull data on header level from external sources (e.g. google doc incl. publication date and language flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# generate header set with unique ids for \"super-items\"\n",
    "header_items_df = generate_header_set(items_df)\n",
    "header_items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## generate header set with unique ids for \"super-items\"\n",
    "#header_items_df = generate_header_set(items_df)\n",
    "#header_items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# add headerID to items_df (drop before join if already existent)\n",
    "if 'headerID' in items_df.columns:\n",
    "    items_df = items_df.drop(columns=['headerID'])\n",
    "items_df = items_df.merge(header_items_df[['title','headerID']], left_on='title', right_on='title',how='left') \n",
    "display(items_df.head())\n",
    "print(f'missing headerIDs in items_df: {items_df[\"headerID\"].isnull().sum()}')\n",
    "\n",
    "# generate lookup table\n",
    "header_items_lookup_df = items_df[['itemID','headerID']].drop_duplicates()\n",
    "print(f'shape of items_df vs. header_items_lookup_df: {items_df.shape} vs. {header_items_lookup_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# add headerID to transactions_df (drop before join if already existent)\n",
    "if 'headerID' in transactions_df.columns:\n",
    "    transactions_df = transactions_df.drop(columns=['headerID'])\n",
    "transactions_df = transactions_df.merge(header_items_lookup_df, left_on='itemID', right_on='itemID',how='left') \n",
    "\n",
    "# inspect results\n",
    "display(transactions_df.head())\n",
    "print(f'# missing headerIDs in transactions_df: {transactions_df[\"headerID\"].isnull().sum()}')\n",
    "print(f'# unique items in transactions_df: {transactions_df[\"itemID\"].nunique()}')\n",
    "print(f'# unique headers in transactions_df: {transactions_df[\"headerID\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DEV] merge with external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# read in preprocessed data\n",
    "g_data = pd.read_feather('../data/processed/gbooks_volumeInfo_pp.feather')\n",
    "g_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cols_pp = ['title', 'authors', 'publisher']\n",
    "\n",
    "# convert all strings to lowercase\n",
    "g_data[cols_pp] = g_data[cols_pp].applymap(lambda s:s.lower() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df.loc[:, 'author_'] = header_items_df.author.map(lambda x: next(iter(x)))\n",
    "header_items_df.loc[:, 'publisher_'] = header_items_df.publisher.map(lambda x: next(iter(x)))\n",
    "header_items_df.loc[:, 'release_date_'] = header_items_df.release_date.map(lambda x: next(iter(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in cols_pp:\n",
    "    \n",
    "    col_cl = col + '_cl'\n",
    "\n",
    "    # add additional col for pp titles\n",
    "    g_data[col_cl] = g_data[col]\n",
    "\n",
    "    # clean strings\n",
    "    if col == 'title':\n",
    "        g_data[col_cl] = g_data[col_cl].apply(remove_nontitle_substrings)\n",
    "    g_data[col_cl] = g_data[col_cl].astype(str).apply(remove_special_characters)\n",
    "    g_data[col_cl] = g_data[col_cl].apply(convert_umlaute)\n",
    "\n",
    "    # reduce all spaces in the articles to single spaces\n",
    "    g_data[col_cl] = g_data[col_cl].apply(remove_duplicate_whitespace)\n",
    "\n",
    "    # print stats\n",
    "    #col_cnt_unique = g_data[col].nunique()\n",
    "    #col_cl_cnt_unique = g_data[col_cl].nunique()\n",
    "    #print(f'# unique {col} (before preprocessing): {col_cnt_unique} / {len(items_df)}')\n",
    "    #print(f'# unique {col} (after preprocessing): {col_cnt_unique} / {len(items_df)}')\n",
    "    #print(f'# reduction in unique {col}: {col_cnt_unique-col_cl_cnt_unique}\\n')\n",
    "    \n",
    "# replace original cols by pre-processed cols\n",
    "g_data = g_data.drop(columns=cols_pp)\n",
    "g_data = g_data.rename(columns={'title_cl': 'title', 'author_cl': 'author', 'publisher_cl': 'publisher'})\n",
    "\n",
    "# remove items with missing title after pre-processing\n",
    "print(f\"remove items with missing/empty title after pp: {(g_data['title']=='').sum()}\")\n",
    "g_data = g_data[g_data['title']!='']\n",
    "\n",
    "# display cleaned df head\n",
    "display(g_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#pd.merge(header_items_df, g_data[['title', 'publisher','publishedDate', 'language', 'maturityRating']], left_on=['title','release_date_'], right_on=['title', 'publishedDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language flag\n",
    "\n",
    "__Idea:__\n",
    "Flag Language of title in order to improve same language recommendations\n",
    "\n",
    "__Lookup Links:__\n",
    "1. [stackoverflow:](https://stackoverflow.com/questions/39142778/python-how-to-determine-the-language) comparison of different language detection modules\n",
    "2. [tds](https://towardsdatascience.com/benchmarking-language-detection-for-nlp-8250ea8b67c) performance evaluation -> recommends __fasttext__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define test strings\n",
    "str_en = \"romeo and juliet: the graphic novel\"\n",
    "str_de = \"sternenschweif. zauberhafter schulanfang\"\n",
    "\n",
    "# define whether to use existing flags and df\n",
    "if not recompute_lg_flg:\n",
    "    items_df = items_df_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### module testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# module detector dict\n",
    "lan_detector = {'ld': 'langdetect', 'gl': 'guess_language', 'lg': 'langid'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### langdetect (=title_ld)\n",
    "[langdetect](https://pypi.org/project/langdetect/)\n",
    "- important: use try-catch block to handle e.g. numerics, urls etc\n",
    "- non-deterministic approach: remember to set seed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langdetect import DetectorFactory, detect\n",
    "from langdetect.lang_detect_exception import LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test detector on sample strings\n",
    "print(detect(str_en))\n",
    "print(detect(str_de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if recompute_lg_flg:\n",
    "    # get start time for performance evaluation\n",
    "    start_time_ld = time.time()\n",
    "\n",
    "    # set seed for reproducability\n",
    "    DetectorFactory.seed = 0\n",
    "\n",
    "    # option 1: pre-calculate list of languages\n",
    "    title_ld = []\n",
    "    for title in items_df['title']:\n",
    "        try:\n",
    "            title_ld.append(detect(title))\n",
    "    #         print(f'{title}: {detect(title)}')\n",
    "        except LangDetectException:\n",
    "            title_ld.append(None)\n",
    "    #         print(f'{title}: \"undefined\"')\n",
    "\n",
    "    # compute execution time\n",
    "    end_time_ld = time.time()\n",
    "    print(f'exection time langdetect: {end_time_ld - start_time_ld} seconds')\n",
    "\n",
    "    items_df['title_ld'] = title_ld\n",
    "\n",
    "    # option 2: use apply and title col\n",
    "    # items_df['title_ld'] = items_df['title'].apply(lambda x: detect(x) if not x.isnumeric() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inspect items w/o language specification -> only numeric !\n",
    "print(f'cnt of items without language flag: {items_df[\"title_ld\"].isnull().sum()}')\n",
    "display(items_df[items_df[\"title_ld\"].isnull()].head(10))\n",
    "\n",
    "# inspect results\n",
    "ld_vc = pd.DataFrame(items_df['title_ld'].value_counts().reset_index())\n",
    "display(ld_vc.transpose())\n",
    "\n",
    "# show barplot with # items with title in given language\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.barplot(x='index', y='title_ld', ax=ax, data=ld_vc, palette=palette_blue).set(\n",
    "    xlabel='languages determined by \"langdetect\"',\n",
    "    ylabel='# items with title in given language'\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### guess_language (=title_gl)\n",
    "\n",
    "- Can detect very short samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from guess_language import guess_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(guess_language(str_en))\n",
    "print(guess_language(str_de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if recompute_lg_flg:\n",
    "\n",
    "    # get start time for performance evaluation\n",
    "    start_time_gl = time.time()\n",
    "\n",
    "    # detect langauge of titles\n",
    "    items_df['title_gl'] = items_df['title'].apply(lambda x: guess_language(x) if not x.isnumeric() else None)\n",
    "\n",
    "    # set 'UNKNOWN' to None\n",
    "    items_df.loc[items_df['title_gl']=='UNKNOWN','title_gl'] = None\n",
    "\n",
    "    # compute execution time\n",
    "    end_time_gl = time.time()\n",
    "    print(f'exection time guess_language: {end_time_gl - start_time_gl} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inspect results\n",
    "gl_vc = pd.DataFrame(items_df['title_gl'].value_counts().reset_index())\n",
    "display(gl_vc.transpose())\n",
    "\n",
    "# show barplot with # items with title in given language\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.barplot(x='index', y='title_gl', ax=ax, data=gl_vc, palette=palette_blue).set(\n",
    "    xlabel='languages determined by \"guess_language\"',\n",
    "    ylabel='# items with title in given language'\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### textblob\n",
    "Requires NLTK package, uses Google -> API blocked with \"HTTP Error 429: Too Many Requests\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.exceptions import TranslatorError"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(TextBlob(str_en).detect_language())\n",
    "print(TextBlob(str_de).detect_language())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# get start time for performance evaluation\n",
    "start_time_tb = time.time()\n",
    "\n",
    "# option 1: pre-calculate list of languages\n",
    "title_tb = []\n",
    "\n",
    "for title in items_df['title']:\n",
    "    try:\n",
    "#         title_tb.append(detect(title))\n",
    "        print(f'{title}: {TextBlob(title).detect_language()}')\n",
    "    except TranslatorError as te:\n",
    "#         title_tb.append(None)\n",
    "        print(f'{title}: {te}')\n",
    "\n",
    "items_df['title_tb'] = title_tb\n",
    "\n",
    "# option 2: use apply\n",
    "# items_df['title_tb'] = items_df['title'].apply(lambda x: TextBlob(x).detect_language() if not x.isnumeric() or  else None)\n",
    "\n",
    "# compute execution time\n",
    "end_time_tb = time.time()\n",
    "print(f'exection time langdetect: {end_time_tb - start_time_tb} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### spacy\n",
    "- [spacy doku](https://spacy.io/universe/project/spacy-langdetect): did not get it working"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### langid (=title_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "langid.classify(str_en)\n",
    "langid.classify(str_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if recompute_lg_flg:\n",
    "\n",
    "    # get start time for performance evaluation\n",
    "    start_time_lg = time.time()\n",
    "\n",
    "    # option 1: pre-calculate list of languages\n",
    "    title_lg = []\n",
    "\n",
    "    for title in items_df['title']:\n",
    "        title_lg.append(langid.classify(title))\n",
    "        print(f'{title}: {langid.classify(title)}')\n",
    "\n",
    "    # compute execution time\n",
    "    end_time_lg = time.time()\n",
    "    print(f'exection time langid: {end_time_lg - start_time_lg} seconds')\n",
    "\n",
    "    # add col to df\n",
    "    items_df['title_lg'] = [t[0] for t in title_lg]\n",
    "\n",
    "    # option 2: use apply\n",
    "    # items_df['title_lg'] = items_df['title'].apply(lambda x: TextBlob(x).detect_language() if not x.isnumeric() or  else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inspect items w/o language specification -> only numeric !\n",
    "print(f'cnt of items without language flag: {items_df[\"title_lg\"].isnull().sum()}')\n",
    "#display(items_df[items_df[\"title_lg\"].isnull()].head(10))\n",
    "\n",
    "# inspect results\n",
    "lg_vc = pd.DataFrame(items_df['title_lg'].value_counts().reset_index())\n",
    "display(lg_vc.transpose())\n",
    "\n",
    "# show barplot with # items with title in given language\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.barplot(x='index', y='title_lg', ax=ax, data=lg_vc, palette=palette_blue).set(\n",
    "    xlabel='languages determined by \"langid\"',\n",
    "    ylabel='# items with title in given language'\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fasttext\n",
    "- official Python binding module by Facebook\n",
    "- problems with installation on windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### module performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compare execution time and items w/o flag\n",
    "if recompute_lg_flg:\n",
    "    lan_detector_eval_df = pd.DataFrame({'execution time [s]': [eval('end_time_'+det.split(\"_\")[1]) - eval('start_time_'+det.split(\"_\")[1]) for det in ['title_ld','title_gl','title_lg']],\n",
    "                                        '#items w/o language flg':[items_df[det].isnull().sum() for det in ['title_ld','title_gl','title_lg']]},\n",
    "                                       index=[det for det in lan_detector.values()])\n",
    "    display(lan_detector_eval_df)\n",
    "\n",
    "# merge results dfs\n",
    "ld_gl_vc = ld_vc.merge(gl_vc, left_on='index', right_on='index', how='outer')\n",
    "ld_gl_lg_vc = ld_gl_vc.merge(lg_vc, left_on='index', right_on='index', how='outer')\n",
    "display(ld_gl_lg_vc.transpose())\n",
    "ld_gl_lg_vc = ld_gl_lg_vc.head(10)\n",
    "\n",
    "# rename columns\n",
    "ld_gl_lg_vc.columns = ['index', 'langdetect','guess_language','langid']\n",
    "\n",
    "# add language name\n",
    "ld_gl_lg_vc['language_name'] = ld_gl_lg_vc['index'].apply(lambda l: pycountry.countries.get(alpha_2=l).name if l != 'en' else 'English')\n",
    "\n",
    "# transform model cols into identifier column for plotting\n",
    "ld_gl_lg_vc = pd.melt(ld_gl_lg_vc, id_vars=[\"index\", \"language_name\"],\n",
    "                  var_name=\"flag_m\", value_name=\"idCnt\")\n",
    "#display(ld_gl_lg_vc)\n",
    "\n",
    "# Draw a nested barplot by language detector\n",
    "sns.set_theme()\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "g = sns.barplot(y=\"language_name\", x=\"idCnt\", hue=\"flag_m\", data=ld_gl_lg_vc, palette=palette_blue, orient='h')\n",
    "g.set(xlabel=\"# itemID\", ylabel = \"\")\n",
    "g.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DEV] Topic Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# creation of sentence embeddings for each category\n",
    "\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load embeddings\n",
    "model = KeyedVectors.load_word2vec_format('../tempData/fetchedData/fasttext.wiki.en.300.vocab_200K.vec')\n",
    "df = pd.read_json('../tempData/processedData/categories_translated.json', typ='series')\n",
    "\n",
    "# create cat strings\n",
    "def cats(row):\n",
    "    words = ''\n",
    "    cat_ins = ''\n",
    "    for char in row:\n",
    "        cat_ins = cat_ins + char\n",
    "        try:\n",
    "            words = words + df.loc[cat_ins].lower() +' '\n",
    "        except:\n",
    "            pass\n",
    "    return words\n",
    "\n",
    "# calculate averaged sentence embedding\n",
    "def calc(words):\n",
    "    summ = 0\n",
    "    i = 0\n",
    "    for word in set(words.split()):\n",
    "        try:\n",
    "            summ = summ + model[word]\n",
    "            i = i+1\n",
    "        except:\n",
    "            pass\n",
    "    return summ/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# loop trough all cats & save as list to be able to save in df\n",
    "avgs = []\n",
    "for i in df.index:\n",
    "    avgs.append([calc(cats(i))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "embs = pd.DataFrame(avgs, index=df.index, columns=['fasttext_emb'])\n",
    "embs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def f(col):\n",
    "    s=0\n",
    "    try:\n",
    "        for e in col:\n",
    "            s = s + embs.loc[e][0]\n",
    "        return s / len(col)\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# create list from set\n",
    "header_items_df.loc[:, 'mt_st_'] = header_items_df.mt_st_cl.map(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df.mt_st_ = header_items_df.mt_st_.map(lambda x: ' '.join(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df.drop('mt_st_cl', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# add categorical embeddings\n",
    "header_items_df['emb_cats'] = header_items_df.apply(lambda x: f(x['mt_st_']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thalia Book Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df.loc[:, 'description_'] = header_items_df.description.map(lambda x: next(iter(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# convert all strings to lowercase\n",
    "header_items_df['description_'] = header_items_df['description_'].apply(lambda s:s.lower() if type(s) == str else s)    \n",
    "header_items_df['description_'] = header_items_df['description_'].apply(remove_nontitle_substrings)\n",
    "header_items_df['description_'] = header_items_df['description_'].astype(str).apply(remove_special_characters)\n",
    "header_items_df['description_'] = header_items_df['description_'].apply(convert_umlaute)\n",
    "header_items_df['description_'] = header_items_df['description_'].apply(remove_next_sign)\n",
    "# reduce all spaces in the articles to single spaces\n",
    "header_items_df['description_'] = header_items_df['description_'].apply(remove_duplicate_whitespace)\n",
    "# display cleaned df head\n",
    "display(header_items_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df.drop('description', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google Translator\n",
    "\n",
    "__Note:__\n",
    "- not suitable as quite unreliable\n",
    "- will block after 20k requests"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "header_items_df[(header_items_df['language'] == {'Italienisch'}) & (header_items_df['description_']!='')]['description_'].apply(translator.translate, src='it',  dest='en' ).apply(getattr, args=('text',))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# header_items_df['descriptions_trans'] = header_items_df['description_'].apply(translator.translate, dest='en' ).apply(getattr, args=('text',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helsinki NLP Translation Model\n",
    "\n",
    "__open:__\n",
    "- clean header translation tags\n",
    "- make translations per language (only select top 5?)\n",
    "- detect whether description even is in source language !!! -> e.g. for Finnisch, most of descriptions seem to be in EN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lan_dist = pd.DataFrame(header_items_20210517.language.value_counts().reset_index())\n",
    "lan_dist_sub = lan_dist[lan_dist['language']>4]\n",
    "print(len(lan_dist_sub))\n",
    "display(lan_dist_sub)\n",
    "sns.barplot(y='index',x='language', data=lan_dist_sub, orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect finnish books\n",
    "finnish_desc = header_items_20210517.loc[(header_items_20210517['language']==\"{'Finnisch'}\") & \n",
    "                                         (header_items_20210517['description']!=\"{'\\\\n'}\"),:]\n",
    "display(finnish_desc.head(5))\n",
    "print(type(finnish_desc))\n",
    "print(len(finnish_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect italian books\n",
    "italian_desc = header_items_20210517.loc[(header_items_20210517['language']==\"{'Italienisch'}\") & \n",
    "                                         (header_items_20210517['description']!=\"{'\\\\n'}\"),:]\n",
    "display(italian_desc.head(5))\n",
    "print(type(italian_desc))\n",
    "print(len(italian_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"it\"\n",
    "target_lang = \"en\"\n",
    "model_name = f'Helsinki-NLP/opus-mt-{lang}-{target_lang}'\n",
    "print(model_name)\n",
    "\n",
    "# Download the model and the tokenizer\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'desc to translate: {len(italian_desc)}')\n",
    "\n",
    "lan_dict = {'it': \"{'Italienisch'}\",\n",
    "           'de': \"{'Deutsch'}\"}\n",
    "\n",
    "translation_hel_lst = []\n",
    "for text in italian_desc['description']:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    gen = model.generate(**inputs)\n",
    "    translation = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "    print(f'original: \\t{text}\\n')\n",
    "    print(f'translation: \\t{translation}\\n\\n')\n",
    "    translation_hel_lst.append(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thalia Recommended age\n",
    "\n",
    "##### bin into children/ teenagers / adults\n",
    "##### fill empty values with adults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# get rid of set structure (only get one value)\n",
    "header_items_df.loc[:, 'recommended_age'] = header_items_df.recommended_age.map(lambda x: next(iter(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df['rec_age'] = header_items_df.recommended_age.str.findall('\\d+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df['rec_age'].apply(lambda x: map(int, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df.recommended_age.str.replace('ab', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df[header_items_df['recommended_age'].str.contains('5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def f(cell):\n",
    "    cell = [int(x) for x in cell]\n",
    "    try:\n",
    "        if max(cell) < 3:\n",
    "            return 'newborn'\n",
    "        elif (min(cell) >=12) & (min(cell) <=18):\n",
    "            return 'teen'\n",
    "        elif (min(cell) > 7) & (min(cell) <12):\n",
    "            return 'child'\n",
    "        elif (min(cell) >=3) & (min(cell) <=7):\n",
    "            return 'small child'\n",
    "        else:\n",
    "            return 'adult'\n",
    "    except:\n",
    "        return 'adult'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df['rec_age_'] = header_items_df['rec_age'].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df.rec_age_.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df.to_csv('header_items_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export of final pre-processed dfs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# replace original cols by pre-processed cols\n",
    "items_df = items_df.rename(columns={'mt_cl': 'mt', 'st_cl': 'st', 'mt_st_cl': 'mt_st'})\n",
    "items_df.head(2)\n",
    "\n",
    "# export items_df\n",
    "items_df.to_csv(items_path_pp)\n",
    "\n",
    "# export transactions_df\n",
    "transactions_df.to_csv(transactions_path_pp)\n",
    "\n",
    "# export header_items_df\n",
    "header_items_df.to_csv(header_items_path_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# export transactions_df\n",
    "transactions_df.to_csv(transactions_path_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "transactions_df.to_csv('transactions_wo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df.to_csv('header_items_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "items_df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "header_items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "426.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n",
    "#!pip install fuzzywuzzy\n",
    "#!pip install python-Levenshtein\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langdetect import detect\n",
    "from difflib import SequenceMatcher\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from re import search\n",
    "import csv\n",
    "\n",
    "\n",
    "df_items = pd.read_csv('items.csv', sep='|', quoting=csv.QUOTE_NONE, error_bad_lines=False)\n",
    "df_transactions = pd.read_csv('transactions.csv', sep = '|')\n",
    "df_evaluation = pd.read_csv('evaluation.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "items_path = '20210525_items_df.csv'\n",
    "path = '20210525_header_items_df.csv'\n",
    "mt_path = 'items_pp.csv'\n",
    "\n",
    "# Some preprocessing\n",
    "items_df = pd.read_csv(items_path, delimiter=',', encoding='utf-8')\n",
    "del items_df['description']\n",
    "del items_df['recommended_age']\n",
    "del items_df['number_pages']\n",
    "\n",
    "#header_df = pd.read_csv(header_path, delimiter=',', encoding='utf-8')\n",
    "\n",
    "header_df = pd.read_csv(path, lineterminator='\\n')\n",
    "del header_df['title']\n",
    "del header_df['author']\n",
    "del header_df['publisher']\n",
    "del header_df['item_lang_en']\n",
    "#items_df = items_df.dropna(axis=0)\n",
    "\n",
    "mt_df = pd.read_csv(mt_path, delimiter=',', encoding='utf-8')\n",
    "\n",
    "#header_df = pd.merge(lang_df, header_df, how='left', left_on=['headerID'], right_on = ['headerID'])\n",
    "items_df = pd.merge(items_df, header_df,  how='left', left_on=['headerID'], right_on = ['headerID'])\n",
    "items_df['mt'] = mt_df['mt']\n",
    "#items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(columns = ['book_id', 'model_id', 'team_id', 'recommendation_1', 'recommendation_2', \n",
    "                                  'recommendation_3', 'recommendation_4', 'recommendation_5'])\n",
    "\n",
    "x = items_df.copy()\n",
    "for index,row in df_evaluation.iterrows():\n",
    "    \n",
    "    items_df = x.copy()\n",
    "    print(items_df.shape)\n",
    "    \n",
    "    #get information from base book\n",
    "    base_book = row['itemID']\n",
    "    print(base_book)\n",
    "    author = items_df['author'][items_df['itemID'] == base_book].to_string(index=False).lstrip()\n",
    "    mtopic = items_df['mt'][items_df['itemID'] == base_book].to_string(index=False).strip(' []')\n",
    "    lang = items_df['language'][items_df['itemID'] == base_book].to_string(index=False).lstrip()\n",
    "    headerID = int(items_df['headerID'][items_df['itemID'] == base_book])\n",
    "\n",
    "    #filter data set according the language of the base book\n",
    "    items_df = filter_on_lang(items_df, lang)\n",
    "    items_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #get different scores\n",
    "    df_authorscores = items_df[['itemID', 'title', 'author']]\n",
    "    df_authorscores['author_score'] = get_authorscores_new(df_authorscores, author)\n",
    "    \n",
    "\n",
    "    df_topicscore = items_df[['headerID','itemID', 'title', 'mt']]\n",
    "    df_topicscore['mtopic_score'] = get_mtopicscores_new(df_topicscore, mtopic)\n",
    "    \n",
    "\n",
    "    df_titlescores = items_df[['itemID', 'title', 'author']]\n",
    "    df_titlescores['title_score'] = get_titlescores(df_titlescores, items_df, base_book)\n",
    "    \n",
    "    result_transactions = recommend_based_on_transactions(df_transactions, df_items, base_book)\n",
    "    \n",
    "    #get final scores and remove duplicates\n",
    "    result = get_totalscore(df_titlescores, df_authorscores, df_topicscore, result_transactions)\n",
    "    result = result[result['headerID'] != headerID]\n",
    "    result.drop_duplicates(subset ='headerID', keep = \"first\", inplace = True)\n",
    "    result = result.sort_values(by='total_score', ascending=False)\n",
    "      \n",
    "    recommendations = result.iloc[0:5,:] \n",
    "    #print(recommendations)\n",
    "    \n",
    "    #write books with top 5 scores to file\n",
    "    final_df = final_df.append({'book_id': row['itemID'],\n",
    "                                'model_id': 'first',\n",
    "                                'team_id': 'dataminerz', \n",
    "                                'recommendation_1': recommendations.iloc[0, 0], \n",
    "                                'recommendation_2': recommendations.iloc[1, 0], \n",
    "                                'recommendation_3': recommendations.iloc[2, 0], \n",
    "                                'recommendation_4': recommendations.iloc[3, 0], \n",
    "                                'recommendation_5': recommendations.iloc[4, 0]}, ignore_index=True)\n",
    "    print(len(final_df))\n",
    "                        \n",
    "\n",
    "final_df.to_csv('rec_final.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several Functions to calculate the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_on_lang(df_item, language):\n",
    "    \n",
    "    if len(df_item[df_item['language'] == lang]) > 5:\n",
    "    \n",
    "        print(len(df_item[df_item['language'] == lang]))\n",
    "        return df_item[df_item['language'] == lang]\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        print(len(df_item))\n",
    "        return df_item\n",
    "    \n",
    "\n",
    "def get_authorscores(df_authorscores, author):\n",
    "\n",
    "    count = 0\n",
    "    hits = 0\n",
    "    for i in df_authorscores['itemID']:\n",
    "        \n",
    "        if pd.isnull(df_authorscores['author'][count]):\n",
    "            df_authorscores.at[count, 'author_score'] = 0\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            if  df_authorscores['author'][count] == author:\n",
    "                df_authorscores.at[count, 'author_score'] = 2\n",
    "                hits += 1\n",
    "            else:\n",
    "                df_authorscores.at[count, 'author_score'] = 0\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    if hits < 3:\n",
    "        \n",
    "        count = 0\n",
    "        for i in df_authorscores['itemID']:\n",
    "\n",
    "            #print(df_authorscores['author'][count])\n",
    "            #print(author)\n",
    "            if pd.isnull(df_authorscores['author'][count]):\n",
    "                df_authorscores.at[count, 'author_score'] = 0\n",
    "            else:  \n",
    "\n",
    "                fuzzratio = fuzz.ratio(author, df_authorscores['author'][count])/100\n",
    "                fuzzpartial = (fuzz.partial_ratio(author, df_authorscores['author'][count]))/100\n",
    "\n",
    "                if fuzzpartial > 0.70 and fuzzratio > 7.0:\n",
    "                    df_authorscores.at[count, 'author_score'] = fuzzpartial + fuzzratio\n",
    "                else:\n",
    "                    df_authorscores.at[count, 'author_score'] = 0\n",
    "                #print(fuzz.partial_ratio(author, df_authorscores['author'][count]))\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        return df_authorscores['author_score']\n",
    "        \n",
    "        \n",
    "    \n",
    "    return df_authorscores['author_score']\n",
    "\n",
    "\n",
    "def get_authorscores_new(df_authorscore, author):\n",
    "    \n",
    "    count = 0\n",
    "    hits = 0\n",
    "    for i in df_authorscores['itemID']:\n",
    "        \n",
    "        if pd.isnull(df_authorscores['author'][count]):\n",
    "            df_authorscores.at[count, 'author_score'] = 0\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            if  df_authorscores['author'][count] == author:\n",
    "                df_authorscores.at[count, 'author_score'] = 2\n",
    "                hits += 1\n",
    "            else:\n",
    "                df_authorscores.at[count, 'author_score'] = 0\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    if hits < 3:     \n",
    "        df_authorscore['author'] = df_authorscore['author'].fillna('')\n",
    "        split_names = df_authorscore['author'].apply(lambda x: x.split(' '))\n",
    "        split_author = author.split(' ')\n",
    "        df_authorscore[split_names.apply(lambda x: len(set(split_author).intersection(set(x)))) >= 2]['author_score'] = 2.0\n",
    "    \n",
    "    return df_authorscores['author_score']\n",
    "\n",
    "\n",
    "def get_authorscores_fuzzy(df_authorscores, author):\n",
    "\n",
    "    count = 0\n",
    "    for i in df_authorscores['itemID']:\n",
    "            \n",
    "        #print(df_authorscores['author'][count])\n",
    "        #print(author)\n",
    "        if pd.isnull(df_authorscores['author'][count]):\n",
    "            df_authorscores.at[count, 'author_score'] = 0\n",
    "        else:  \n",
    "            \n",
    "            fuzzratio = fuzz.ratio(author, df_authorscores['author'][count])/100\n",
    "            \n",
    "            if (fuzz.partial_ratio(author, df_authorscores['author'][count])/100) > 0.70 and (fuzzratio) > 0.7:\n",
    "                df_authorscores.at[count, 'author_score'] = 1 + fuzzratio\n",
    "            else:\n",
    "                df_authorscores.at[count, 'author_score'] = 0\n",
    "            #print(fuzz.partial_ratio(author, df_authorscores['author'][count]))\n",
    "            \n",
    "        count += 1\n",
    "    \n",
    "    return df_authorscores['author_score']\n",
    "\n",
    "def get_mtopicscores_new(df_mtopicscores, mtopic):\n",
    "    def get_mt_similarity(mt_sample, mtopic):\n",
    "        if not mt_sample:\n",
    "            return 0\n",
    "        else:\n",
    "            \n",
    "            return SequenceMatcher(None, str(mt_sample).strip('[]'), str(mtopic)).ratio()\n",
    "            \n",
    "    \n",
    "    mt_score = df_mtopicscores[\"mt\"].apply(lambda mt_sample: get_mt_similarity(mt_sample, mtopic))\n",
    "    return mt_score\n",
    "\n",
    "def get_mtopicscores_emb(df_mtopicscores, emb):\n",
    "\n",
    "    emb_items = df_mtopicscores['emb_cats']\n",
    "    \n",
    "    cos_similarity_topics = map(lambda x: cosine_similarity(emb, x), emb_items)\n",
    "    \n",
    "    output = list(cos_similarity_topics)\n",
    "        \n",
    "    \n",
    "    return np.array(output).ravel()\n",
    "\n",
    "\n",
    "def get_titlescores (df_titlescores, items_df, base_book):\n",
    "    \n",
    "    #Creating tfidf-matrix\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_items = tfidf_vectorizer.fit_transform((df_titlescores['title'].apply(lambda x: np.str_(x))))\n",
    "\n",
    "    #Retrieving book for recommendations\n",
    "    test_tfidf = tfidf_items[items_df['itemID'] == base_book]\n",
    "\n",
    "    #Retrieving most similar books\n",
    "    cos_similarity_tfidf = map(lambda x: cosine_similarity(test_tfidf, x), tfidf_items)\n",
    "    output = list(cos_similarity_tfidf)\n",
    "    \n",
    "    return np.array(output).ravel()\n",
    "    \n",
    "def get_totalscore(df1, df2, df3, df_trans):\n",
    "\n",
    "    result = pd.DataFrame(columns=['itemID', 'headerID', 'title', 'author', 'mt','title_score', 'author_score', 'mtopic_score', 'trans_score', 'total_score'])\n",
    "    \n",
    "    result['itemID'] = df1.iloc[:,0]\n",
    "    result['headerID'] = df3.iloc[:,0]\n",
    "    result['title'] = df3.iloc[:,2]\n",
    "    result['author'] = df1.iloc[:,2]\n",
    "    result['mt'] = df3.iloc[:,3]\n",
    "    result['title_score'] = df1.iloc[:,3]\n",
    "    result['author_score'] = df2.iloc[:,3]\n",
    "    result['mtopic_score'] = df3.iloc[:,4]\n",
    "    result['trans_score'] = 0\n",
    "    \n",
    "    count = 0\n",
    "    for i in df_trans:\n",
    "        \n",
    "        result['trans_score'][result['itemID'] == df_trans[count]] = 1\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    result['total_score'] = result['title_score'] + result['author_score'] + result['mtopic_score'].astype(float) + result['trans_score'].astype(float)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_based_on_transactions(df_transactions, df_items, item_id, sort_by=\"sum\",\n",
    "                                    max_number_recommendation=10, verbose=False):\n",
    "    \n",
    "    original_book = convert_id_to_name(df_items, item_id)\n",
    "    original_title = original_book[[\"title\"]].values[0][0]\n",
    "    original_author = original_book[[\"author\"]].values[0][0]\n",
    "    recommendation_id = []\n",
    "\n",
    "    if original_book.empty:\n",
    "        sys.exit(\"ITEM ID NOT FOUND!\")\n",
    "    if verbose:\n",
    "        print(\"Find recommendations based on: \")\n",
    "        print(\"{} by {}.\".format(original_title, original_author))\n",
    "        print(\"\\n\")\n",
    "        print(\"We recommend: \")\n",
    "    session_ids = find_all_sessions_id(df_transactions, item_id)\n",
    "    itemID_rows = find_all_item_id(df_transactions, session_ids)\n",
    "    item_properties = find_number_click_basket_order(itemID_rows)\n",
    "\n",
    "    if sort_by == \"sum\":\n",
    "        summed_item_properties = sum_click_basket_order(item_properties)\n",
    "        sorted_itemID = sorted(summed_item_properties, key=summed_item_properties.get, reverse=True)\n",
    "\n",
    "        number_recommendation = 0\n",
    "        for single_itemID in sorted_itemID:\n",
    "            rank_book = convert_id_to_name(df_items, single_itemID)\n",
    "            if single_itemID == item_id or compare_strings(original_title, rank_book[[\"title\"]].values[0][0]):\n",
    "                continue\n",
    "            if number_recommendation < max_number_recommendation:\n",
    "                recommendation_id.append(single_itemID)\n",
    "                if verbose:\n",
    "                    print(\"{}. {} by {}.\".format(number_recommendation + 1,\n",
    "                                                 rank_book[[\"title\"]].values[0][0],\n",
    "                                                 rank_book[[\"author\"]].values[0][0]))\n",
    "                number_recommendation += 1\n",
    "            else:\n",
    "                break\n",
    "        if verbose:\n",
    "            for i in range(max_number_recommendation):\n",
    "                if i + 1 >= number_recommendation + 1:\n",
    "                   print(\"{}. Not enough data to give recommendation\".format(i + 1))\n",
    "\n",
    "    else:\n",
    "        sys.exit(\"INCORRECT ARGUMENT FOR sort_by!\")\n",
    "\n",
    "    return recommendation_id\n",
    "\n",
    "\n",
    "def find_all_sessions_id(df, item_id):\n",
    "    session_ids = df.loc[df['itemID'] == item_id][\"sessionID\"].to_list()\n",
    "    return session_ids\n",
    "\n",
    "\n",
    "def find_all_item_id(df, session_ids):\n",
    "    return df.loc[df['sessionID'].isin(session_ids)]\n",
    "\n",
    "\n",
    "def find_number_click_basket_order(itemID_rows):\n",
    "    item_properties = {}\n",
    "\n",
    "    for index, row in itemID_rows.iterrows():\n",
    "        if row['itemID'] in item_properties.keys():\n",
    "            item_properties[row['itemID']][0] += row['click']\n",
    "            item_properties[row['itemID']][1] += row['basket']\n",
    "            item_properties[row['itemID']][2] += row['order']\n",
    "        else:\n",
    "            item_properties[row['itemID']] = [row['click'], row['basket'], row['order']]\n",
    "    return item_properties\n",
    "\n",
    "\n",
    "def sum_click_basket_order(item_properties):\n",
    "    summed_item_properties = {}\n",
    "    for key, value in item_properties.items():\n",
    "        summed_item_properties[key] = sum(value)\n",
    "\n",
    "    return summed_item_properties\n",
    "\n",
    "\n",
    "def convert_id_to_name(df_items, itemID):\n",
    "    return df_items.loc[df_items['itemID'] == itemID]\n",
    "\n",
    "\n",
    "def compare_strings(text1, text2):\n",
    "    test1_wh_sw = remove_stopwords(text1)\n",
    "    test2_wh_sw = remove_stopwords(text2)\n",
    "\n",
    "    return test1_wh_sw.lower() == test2_wh_sw.lower()\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    return \" \".join(tokens_without_sw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
